---
title: "Does parliament matter for Australian government policy?"
author: Rohan Alexander
date: "`r format(Sys.time(), '%d %B %Y')`"
# output: 
#   pdf_document: 
#     latex_engine: xelatex
output:  
  bookdown::pdf_document2:
    fig_caption: yes
    latex_engine: xelatex

bibliography: bibliography.bib
fontsize: 12pt
number_sections: yes
#geometry: margin=2in
  # :
  #   toc: true
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')
```

# Introduction
<!--
Australian federal government spending is expected to account for about 26 per cent of GDP in 2016-17, and over the past 40 years it has averaged 24.9 per cent (2016-17 Budget Paper No.1, Table 10). Government spending is an important part of GDP but we do not have a good understanding of how governments decide on their eventual spending decision, or even how much of an impact a government has on government spending. Anecdotally, the answer seems to be considered obvious, but no one seems to be able to agree on what that obvious answer is. That is what this paper is about.

From a political institutions perspective, a strict separation of powers suggests that Australia's parliament would be the preeminent institution determining government spending. Hansard is the written record of what was said in parliament. So, it should be possible to analyse Hansard to see if parliament does in fact determine government spending. In Australia, the members of the executive -- cabinet -- must also be members of the legislative -- parliament. So even if spending were determined by the executive then Hansard should have explanatory power.

A literature in political economy examines government spending as an outcome of politicians trying to, for instance: get re-elected, fulfil campaign pledges, or express partisan preferences (for a summary see, for instance, @Benassy-Quere2010 [p. 9]). And it is undeniable that politicians in a liberal democracy have considerable scope to allocate spending at the level of hundreds-of-thousands, or millions, of dollars. But it is not so clear that they have an impact on spending on a macroeconomic level. In order for that political economy literature on why politicians affect government spending to be important, it must be established whether they even affect it to a significant degree.

This paper is at the intersection of positive economics, in that it looks at the impact of policy on the economy, and political economy, in that it examines how those policy decisions are made. In this paper, I analyse 'who said what' in Australia's parliament and how politicians voted. I then use this to explain disaggregated government spending. I use Hansard as the record of what was said in parliament and who said it. -->

Hansard provides a record of all debates in the Commonwealth parliament. The stability of Australia's government over this period, and the division of power between the federal and state governments make Australia especially well-disposed for analysis. Also, @ButlinBarnardPincus1982 [p. 4] argue, Australian governments have played a larger role 'throughout virtually the whole of Australian history in all decision-making processes' and have been a 'much more significant' influence on the economy than in comparable countries.

Using natural language processing I attribute a topic to each statement in Hansard. I then classify whether the statement is objective or subjective, and whether subjective statements are positive or negative. This creates a dataset of the opinions that parliamentarians held on various topics over time. From other sources, I add information about the role of the parliamentarian at the time they made that statement. **[Expand/amend as completed.]**

Government spending data is available from various sources and at various levels of disaggregation. Initially, I just examine total government spending, but I also disaggregate the spending into various areas. **[Expand/amend as completed.]**

<!--
The contribution of this paper is to bring separate threads in existing economics literatures to analyse whether what is said in parliament affects government spending. If parliament in a liberal democracy is essentially just a 'talk shop' with little actual impact on government spending, then analysis of government spending would need to focus on other institutions. Alternatively, if parliament plays an important role, then further analysis of how these government spending decisions occur is needed. Either way, there are implications for our understanding of the institutions that determine about a quarter of GDP.
-->





# Literature
<!--
This paper fits into the traditions of both positive economics and political economy and builds on existing economics literatures. The Hansard records of other countries have been analysed using natural language processing, albeit not for this purpose. And the Australian Hansard record has been examined by hand. Similarly, while there has been much analysis of the impact of different political institutions and the efficiency of government spending, there has not been much examination of what happens within parliament.
-->

## Hansard analysis
@Whyte2014 uses Canadian and UK Hansard records to analyse whether parliamentary disruptions have increased in those countries since 1994. Python is used to process Hansard XML files and then the frequency of interruptions by the members of parliament or calls for order from the speaker is determined. (Using similar methods @Willis2017 examines the UK Climate Change Bill to understand how politicians understand and articulate climate change.) @Whyte2014 is limited to frequency comparisons because the digitised Canadian Hansard was, at that time, only available to 1993. Subsequently the Canadian Hansard was digitised, as described by @BeelenEtc2017.

That digitisation of the Canadian Hansard allowed @Whyte2017 to examine the same question over a longer time-frame -- 1926 to 2015 -- and with more sophisticated methods. @Whyte2017 codes Hansard so that a speech fragment is one of five states: a government backbencher speaking; a minister speaking; an opposition member speaking; an interruption; or a statement by the Speaker of the House. The state of a speech fragment is then considered as an explanatory variable for the state of the next speech fragment in a multinomial logit model. The dataset has 3.1 million speeches of which about five per cent are interruptions. Future work is planned to extend the analysis to consider tone and context.

@Duthie2016 analyse UK Hansard records using natural language processing to examine which politicians made supportive or aggressive statements toward other politicians between 1979 and 1990. For each statement four tags were applied: source-person, target-person, support, attack. @Duthie2016 use the Stanford Named Entity Recognizer to extract statements that contain names, organisations and locations, and then the Stanford Part-of-Speech Tagger to extract statements that contain pronouns. @Duthie2016 add domain specific rules to account for the language nuances of the parliament, and then use the Stanford Classifier Library, which uses supervised learning techniques, and the Sentiment Word Lexicon which contains 2,006 words tagged as positive and 4,738 words tagged as negative to analyse sentiment. They find that changes in language are associated with periods of political change, for instance when there was infighting in the Labour Party and when Margaret Thatcher became prime minister.
 
@GrijzenhoutMarxJijkoun2014 examine various natural language processing options in the context of the Dutch House of Representatives. After hand coding a corpus, they compare the results of various automated processes. They find that machine-learning algorithms perform the best and provide a helpful summary of many different processes. Dutch parliamentary records are particularly rich because DutchParl provides a digital corpus of all parliamentary documents, as detailed in @MarxSchuth2010

More generally, Hansard records have been analysed for various purposes. For instance, @Rasiah2010 examines Hansard records for the Australian House of Representatives to examine whether politicians attempted to evade questions about Iraq during February and March 2003. @GansLeigh2012 examined Australian Hansard records by hand to associate mentions by politicians of certain public intellectuals with neutral or positive sentiment. @Willis2017 looks at a corpus of 97,000 words from UK Hansard records to try to understand how politicians understand climate change. @SealeyBates2016 examine the responses of British Prime Ministers to find that their focus on cognitive and communicative processes. @Graham2016 examines Hansard records for New Zealand between 1890 and 1950 to analyse unparliamentary language. And @WilkersonSmithStramp2015 looks for similarities in how legislation is written in the US.

More recently, @Curran2017 consider New Zealand parliament's Hansard to model the topics discussed between 2003 and 2016. They use LDA to detect themes that have been talked about.


<!--
## Government spending
### International
@Frey1978 [p. 5] lamented that '(e)conomics and politics depend closely and intimately on one another in a modern society. Is this fact duly accounted for in current economic thought?'. Since then there has been a push to integrate politics into economic research, and this paper fits into that broad research agenda. 

Generally, from a positive economics approach, there has been much work explaining aggregate or disaggregated government spending on the basis of various explanatory variables. For instance, @DiMatteoDiMatteo1998 explain Canadian government expenditure on health care over the period 1965 to 1991 as a function of variables such as provincial income, proportion of the population over 65 and federal transfers. Similarly, @Mauro1998 examines whether corruption can explain government expenditure across a cross-section of countries.

In a similar style, although from a political economy approach, @PerssonRolandTabellini2007, @Tabellini2000 and @PerssonTabellini2010 link the level of government spending with political economy considerations such as electoral rules and political institutions. They are interested in how fiscal policy, in terms of the features of government spending and taxation, is determined. Electoral rules and political institutions should impact the number of parties, which should influence government formation and fiscal policy. The focus of political economy on institutions is a feature of @North1985. More recently, @AcemogluJohnsonRobinson2005 [p. 392] conduct their analysis within a framework where political institutions and the distribution of resources imply economic and political institutions, which in turn imply economic performance and the distribution of resources. 


### Australia
@Coulson2016 has a similar interest to the broad question in this paper: to what extent is parliament a genuine actor in the policy process. To help answer this @Coulson2016 examines budget estimates committees using Hansard records for three Australian states: Queensland; Victoria; and South Australia, over five budget years: 2011-12 to 2015-16. The analysis is done by hand which limits the extent of the records that could be examined in a reasonable time. If parliament, or in this case, budget estimates committees (which are made up of parliamentarians), are important then the topics that the parliamentarians discuss should be related to the budget documents that were ostensibly the focus of the committee at that time. @Coulson2016 finds that approximately half of the topics raised by the parliamentarians were related to the budget documents being examined and that the parliamentarians focussed more on the budget measures than government policy or operational matters.

This paper also fits into an existing literature on examining the role of government in Australia's economic history. For instance, @Boot1998, @Frost2000 and @Boot2000 focus on the role of government in the 1800s. While @Boot1998 and @Frost2000 disagree about the specific contribution, they agree that political concerns did influence government decision-making, and it was not just economic efficiency.

-->



# Data

## Hansard

### Overview
In 1901 six former British colonies federated to form Australia. The first parliament sat on 9 May 1901, and a written record of what has been said in parliament is available since that time. Based on the UK parliament, this record is called Hansard. 

The mission statement of Hansard is: 'To provide an accurate, substantially verbatim account of the proceedings of the parliament and its committees which, while usually correcting obvious mistakes, neither adds to nor detracts from the meaning of the speech or the illustration of the argument.' (http://www.aph.gov.au/Parliamentary_Business/Hansard, accessed 12 September 2017).

While Hansard is not necessarily verbatim, it should be close enough for the purposes of this paper. For instance, @Mollin2008 found that in the case of the UK Hansard the differences would only affect specialised linguistic analysis. @Edwards2016 examined Australia, New Zealand and the UK, and found that changes were usually made by those responsible for creating the Hansard record, instead of the parliamentarians. Both these findings provide reassurance that differences between Hansard and a verbatim record would not be meaningful for this paper.

### Initial Hansard dataset
The Australian Parliament House makes Hansard available online (http://www.aph.gov.au/Parliamentary_Business/Hansard). Tim Sherratt at the University of Canberra has scraped the website and makes the entire Hansard for 1901 to 1980 available in XML files (http://historichansard.net/). After downloading Sherratt's XML files, some initial exploration can be done to illustrate the structure and content of the files. 

Sherratt has each day's Hansard in a separate XML file and those are grouped in folders by year. For instance, "19480311_reps_18_196.xml" is the Hansard record for 11 March 1948 and is within the 1948 folder. 

The content of each XML file can be searched for its \<session.header> tag to get information such as the date, parliament and session numbers, etc for that file. Speeches occur within \<debate> tags, with information about the broad content and type of that particular debate contained within child \<debateinfo> and \<subdebateinfo> tags. Within each particular set of \<debate> tags there can be many \<speech> tags. Typically, toward the start of a <speech> tag there will be a \<talk.start> tag, followed by information about the talker contained within <talker> tags. Examples of the data contained within \<talker> tags include: page.no, name.id, electorate, party, role, in.gov, first.speech, and actual name.

While it is possible to analyse the XML files themselves, converting the XML files into rectangular tables makes it easier to use a wider variety of R packages. The other advantage is that it makes some of the errors more obvious. The rectangular dataframe has almost three million obversations and 21 variables. Further information on each of the variables:

- debate_title: **[ADD DESCRIPTION]**
- debate_page.no: **[ADD DESCRIPTION]**
- debate_type: **[ADD DESCRIPTION]**
- subdebate_title: **[ADD DESCRIPTION]**
- subdebate_page.no: **[ADD DESCRIPTION]**
- speakerID: **[ADD DESCRIPTION]**
- speaker_name_meta: **[ADD DESCRIPTION]**
- speaker_name_display: **[ADD DESCRIPTION]**
- page_number: **[ADD DESCRIPTION]**
- electorate: **[ADD DESCRIPTION]**
- party: **[ADD DESCRIPTION]**
- role: **[ADD DESCRIPTION]**
- in_gov: A 
- first_speech: **[ADD DESCRIPTION]**
- statement: **[ADD DESCRIPTION]**
- statement_type: **[ADD DESCRIPTION]**
- dates: **[ADD DESCRIPTION]**
- parliament.no: **[ADD DESCRIPTION]**
- session.no: **[ADD DESCRIPTION]**
- period.no: **[ADD DESCRIPTION]**
- chamber: **[ADD DESCRIPTION]**

The Hansard records that Sherratt scraped were messy and this carried through to the Sherratt XML files and then the rectangular table. While there are implications both ways, overall it is expected that three million is an overstatement of the number of records. 

I examined the first 10,000 rows row-by-row and identified two types of errors that increased the number of observations:

1. In certain cases rows neeed to be pushed into the one above as they contain no additional information other than speech text that is just a continuation of the speech text in the earlier row. For instance, **[INSERT EXAMPLE.]** Out of the first 10,000 rows this occurs on X rows: 1139, 1289, 1290, 1291, 1292, 1293, 1294, 1309, 1350, 1351, 1426, 1433, 1505.
2. There are some rows where Hansard text has been identified as being spoken, but it is a stage direction. For instance, **[INSERT EXAMPLE.]** Examining the original Hansard documents suggests this occured during the digitisation process because it would have been difficult to automatically distinguish spoken text from direction. Out of the first 10,000 rows this occurs on X rows: 10, 50, 65, 68, 69, 76, 79, 80, 83, 84, 85, 99, 100, 104, 105, 329, 336, 925, 926, 1407, 1408, 1412, 1415, 1422.

I identified one type of error that reduces the number of statements made by a particular politican, but not the overall number of observations:

1. Sometimes, particuarly in earlier years the identity of the speaker needs to be pulled out of first part of the text. This means that it has not been attributed to a particular politician although it is still a row in the table. For instance, **[INSERT EXAMPLE]**. Out of the first 10,000 rows this occurs on X rows: 1158, 1243, 1249, 1274, 1307, 1314, 1322, 1367, 1372, 1419, 1454, 1466, 1470, 1476, 1497

Finally, I identified one type of error where text has not been included in the table, and will slightly increase the number of observations:

1. **[INSERT DETAILS OF THAT EXAMPLE]**.

The impact of the first three errors should be able to be mitigated using statistical learning classification. While this will not find all the examples, it should find many. The final error will need to be addressed by improved parsing of the XML.


### Addressing the known issues
**[TBD]**

### Fixing the spelling
https://stackoverflow.com/questions/24443388/stemming-with-r-text-analysis/24454727#24454727

### Preliminary summary
Notwithstanding the known issues identified above, some initial summary statistics can be created for the Hansard dataset. 

#### Number of parliamentarians
There are 757 unique speaker IDs. While some members have 

speaker_name_meta - 773
speaker_name_display - 3313



#### Turnover of politicians
Athanassios Gouglas examines the turnover of Australian politicians using information from parliamentary yearbooks. Gouglas has many measures of turnover and detailed analysis of their implications. It is possible to estimate the number of new MPs each year using Hansard records. While there are too many caveats for this to be an appropriate alternative to Gouglas' yearbook appropriate, comparing the results here with Gouglas is a useful check on the Hansard data. (An example of a caveat is that it is possible for a politician to be in parliament but not speak. That politican would not show up in the Hansard record, but would show up in the yearbook.)

I started with all Hansard statements for 1901-1980. Then I grouped the statements by the politician that was speaking. Then I took the year of the first speech for each politician. Finally I summarised that by year. 



The graph of this, along with a best fit curve, is in Gouglas_turnover.pdf (attached). The y-axis is the proportion of the seats that were taken up by new politicians that year.

After this, I realised that my estimate of the average was being held down by non-election years. So I looked up the years of elections in Australia and then tried to work out when the politicians elected in that election would have been speaking for the first time in parliament. (For instance, if the election was in December of a year then most probably wouldn’t have spoken for the first time until the following year so the year of interest is the following year - there’s a bit of error introduced here.) Combining that with the earlier data meant that I could set all other years to NA. 

The graph of that, along with the best fit curve, is in Douglas_turnover_select.pdf (attached).

I’ve attached the summary data by year as a CSV in case you’re interested. ‘Seats’ is the total number of seats available in parliament, ‘important’ is a dummy for whether that year is likely to have lots of new politicians from an election, ‘new_hansard_speakers’ is the number of new politicians recorded as making statements in that year, ‘gouglas_new’ is the new speakers as a proportion of the seats; ‘gouglas_new_important’ adds NA to a year if it’s not important (there’s going to be a little bit of error introduced here, but ballpark figures should be right). BTW I dropped 1901 because that was the first year so it was skewing the best fit!










Words: Total number of words?

In the ten years from Federation, did they talk about and resolve whatever it was that were the reasons for Federation?

'Customs and excise duties were to become almost the federal government's sole revenue source for the first 10 years of Federation, and a major source in the years leading to World War 2. They were the subject of intense debate in teh parliament.' @Wilson2015 [p. 333]. Is this true? Etc etc, there's other claims about debate on this page.















### Sentiment analysis
**[TBD.]**


## Government spending
**[Expand/amend - need longer and disaggregated series - ABS Year Book.]**

# Analysis
**[TBD.]**

# Outstanding issues and future steps
- Add Senate Hansard
- Add Committees Hansard
- Consider statements in newspapers (Trove)

# Conclusion
**[TBD.]**


\newpage

# (APPENDIX) Appendix {-} 


# Topic analysis
## Overview
Each statement in Hansard needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of @Blei2003latent, as implemented by the R package 'topicmodels' by @Grun2011. 

The key assumption behind the LDA method is that each statement, 'a document', in Hansard is made by a speaker who decides the topics they would like to talk about in that document, and then chooses words, 'terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified *ex ante*; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in Hansard group themselves to define topics.

## Document generation process
As applied to Hansard, the LDA method considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the speaker then chooses appropriate words to use for each of those topics. 

More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \@ref(fig:topicsoverdocuments)).

```{r topicsoverdocuments, echo=FALSE, out.width='.49\\linewidth', fig.cap = "Probability distributions over topics", fig.show = 'hold'}
document_1 <- tibble(
  Topics = c(
  "topic 1", "topic 2", "topic 3", "topic 4", "topic 5"
  ),
  Probability = c(0.40, 0.40, 0.1, 0.05, 0.05)
  )

document_2 <- tibble(
  Topics = c(
  "topic 1", "topic 2", "topic 3", "topic 4", "topic 5"
  ),
  Probability = c(0.01, 0.04, 0.35, 0.20, 0.4)
  )  

par(mfrow=c(2,1))
ggplot(document_1, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 1") +
  coord_cartesian(ylim = c(0, 0.4))

ggplot(document_2, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \@ref(fig:topicsoverterms)).

```{r topicsoverterms, echo=FALSE, out.width='.49\\linewidth', fig.cap = "Probability distributions over terms", fig.show = 'hold'}
topic_1 <- tibble(
  Terms = c(
  "immigration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"
  ),
  Probability = c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2)
  )

topic_2 <- tibble(
  Terms = c(
  "immigration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"
  ),
  Probability = c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)
  )  

# par(mar = c(4, 4, 0.1, 0.1))
ggplot(topic_1, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 1") +
  coord_cartesian(ylim = c(0, 0.4))
ggplot(topic_2, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

Following @BleiLafferty2009, @blei2012 and @GriffithsSteyvers2004, the process by which a document is generated is more formally considered to be:

1. There are $1, 2, \dots, k, \dots, K$ topics and the vocabulary consists of $1, 2, \dots, V$ terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the $k$th topic is $\beta_k$. Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter $0<\eta<1$ is used: $\beta_k \sim \mbox{Dirichlet}(\eta)$.[^Dirichletfootnote] Strictly, $\eta$ is actually a vector of hyperparameters, one for each $K$, but in practice they all tend to be the same value.
2. Decide the topics that each document will cover by randomly drawing distributions over the $K$ topics for each of the $1, 2, \dots, d, \dots, D$ documents. The topic distributions for the $d$th document are $\theta_d$, and $\theta_{d,k}$ is the topic distribution for topic $k$ in document $d$. Again, the Dirichlet distribution with the hyperparameter $0<\alpha<1$ is used here because usually a document would only cover a handful of topics: $\theta_d \sim \mbox{Dirichlet}(\alpha)$. Again, strictly $\alpha$ is vector of length $K$ of hyperparameters, but in practice each is usually the same value.
3. If there are $1, 2, \dots, n, \dots, N$ terms in the $d$th document, then to choose the $n$th term, $w_{d, n}$:
    a. Randomly choose a topic for that term $n$, in that document $d$, $z_{d,n}$, from the multinomial distribution over topics in that document, $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$. 
    b. Randomly choose a term from the relevant multinomial distribution over the terms for that topic, $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$.
    
[^Dirichletfootnote]: The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, $\eta=1$, it is equivalent to a uniform distribution. If $\eta<1$, then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as $\eta$ decreases. A hyperparameter is a parameter of a prior distribution.

Given this set-up, the joint distribution for the variables is (@blei2012, p.6):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$

Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over $\beta_{1:K}$ and $\theta_{1:D}$, given $w_{1:D, 1:N}$. This is intractable directly, but can be approximated (@GriffithsSteyvers2004 and @blei2012).

## Analysis process
After the documents are created, they are all that we have to analyse. The term usage in each document, $w_{1:D, 1:N}$, is observed, but the topics are hidden, or 'latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \@ref(fig:topicsoverdocuments) or \@ref(fig:topicsoverterms). In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (@SteyversGriffiths2006). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (@blei2012, p.7): 
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$

The initial practical step when implementing LDA given a corpus of documents is to remove 'stop words'. These are words that are common, but that don't typically help to define topics. There is a general list of stop words such as: "a"; "a's"; "able"; "about"; "above"... An additional list of words that are commonly found in Hansard, but likely don't help define topics is added to the general list. These additions include words such as: "act"; "amendment"; "amount"; "australia"; "australian"; "bill"... A full list can be found in Appendix \@ref(hansard-stop-word). We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.

After the dataset is ready, the R package 'topicmodels' by @Grun2011 can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following @SteyversGriffiths2006 and @Darling2011, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with $\alpha = \frac{50}{K}$ and $\eta = 0.1$  (@SteyversGriffiths2006 recommends $\eta = 0.01$), where $\alpha$ refers to the distribution over topics and $\eta$ refers to the distribution over terms (@Grun2011, p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (@Grun2011, p.6):
$$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$
where $z'_{d, n}$ refers to all other topic assignments; $\lambda'_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$; $\lambda'_{.\rightarrow k}$ is a count of how many other times that any term has been assigned to topic $k$; $\lambda'^{(d)}_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$ in that particular document; and $\lambda'^{(d)}_{-i}$ is a count of how many other times that term has been assigned in that document. Once $z_{d,n}$ has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (@SteyversGriffiths2006). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate. 


## Warnings and extensions
The choice of the number of topics, *k*, affects the results, and must be specified *a priori*. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for *k* and then picking an appropriate value that performs well.

One weakness of the LDA method is that it considers a 'bag of words' where the order of those words does not matter (@blei2012). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking.









\newpage

# Hansard stop word
The following words were added to the usual list of stop words: "1", "2", "act", "amendment", "amount", "australia", "australian", "bill", "board", "cent", "clause", "commission", "committee", "commonwealth", "countries", "country", "day", "deal", "debate", "department", "desire", "duty", "gentleman", "government", "honorable", "honourable", "house", "increase", "labor", "labour", "leader", "legislation", "matter", "minister", "money", "national", "opposition", "parliament", "party", "people", "policy", "position", "power", "prime", "proposed", "public", "question", "regard", "report", "service", "situation", "south", "speaker", "statement", "support", "system", "time", "united", "vote", and "wales".

\newpage

# References

