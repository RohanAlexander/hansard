---
output:
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    number_sections: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
title: "The Effect of Events on Australian Parliamentary Discussion (1851--2017)"
thanks: "Thank you to John Tang, Zach Ward, Tim Hatton, Martine Mariotti, Tianyi Wang, Matt Jacob, Leslie Root, Jill Sheppard, Matthew Kerby, Chris Cochrane and Dan Simpson for their helpful suggestions; and to the UC Berkeley Demography Department for the use of their computing resources. Comments on the `r format(Sys.time(), '%d %B %Y')` version of this paper welcome at: rohan.alexander@anu.edu.au."
author:
- name: Monica Alexander
  affiliation: University of Toronto
- name: Rohan Alexander
  affiliation: Australian National University
abstract: "We systematically analyse how parliamentary discussion changes in response to different types of events in Australian history. We first create a dataset of what was said in Australian state and federal parliaments from the mid-1800s through to 2017 based on available public records. To reduce the dimensionality of this dataset we use a topic model that allows for correlation and steady changes over time. We analyse the effect of various events using a Bayesian hierarchical Dirichlet model. We find that: 1) elections are associated with topic changes only when the party in power changes; 2) a change in government can be associated with topic changes even if the party in power does not change; 3) economic events, such as financial crises, have less significant and persistent effects than other events such as terrorist attacks. Our findings have implications for how we think about the longer-term trajectory of government policymaking as the media cycle becomes increasingly focused on short-term events."
keywords: "text analysis, politics, Australia"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
toc: FALSE
# spacing: double
bibliography: ../bibliography.bib
biblio-style: apsr
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(kableExtra)
library(knitr)
library(tidyverse)

yearsUsed <- read_csv(here::here("outputs/misc/years_used.csv"), col_types = cols()) %>% 
  replace_na(list(Parliament = ""))
elections <- read_csv(here::here("inputs/misc/misc_elections_data.csv"), col_types = cols())
governments <- read_csv(here::here("inputs/misc/change_of_pm.csv"), col_types = cols())
keyEvents <- read_csv(here::here("inputs/misc/key_events.csv"), col_types = cols())
significances <- read_csv(here::here("outputs/misc/significance.csv"), col_types = cols())
top_words <- read_csv2(here::here("outputs/misc/top_words.csv"), col_types = cols())
missing_days <- read_csv(here::here("outputs/misc/missing_days.csv"), col_types = cols())
hansard_sources <- read_csv(here::here("outputs/misc/hansard_sources.csv"), col_types = cols())


```


# Introduction
Government policy is partly driven by parliamentary discussion. Conversely, that same discussion can indicate a government's priorities. But major events--both expected, such as an election, and unexpected, such as a recession or terrorist attack--can affect the course of parliamentary discussion. For instance, think of how a new government often goes to some trouble to appear different to the one they replace, or how events such as the 9/11 attacks altered government priorities.

In this paper we examine text records of what was said in Australian state and federal parliaments. Our earliest record is from 1851 and we consider the time period through to 2017. 
<!-- The dataset is detailed in Section \@ref(data).  -->
We use a topic model for dimensionality reduction and to allow for correlation between topics and steady changes over time. We then analyse the topics using a Bayesian hierarchical Dirichlet model to examine changes at various types of events. These events include: changes in government or elections; changes in economic conditions; and other significant events such as the 9/11 attacks. 
<!-- The models are detailed in Section \@ref(model). -->

<!-- Section \@ref(results) details our results, but broadly  -->
We find: 1) elections are associated with topic changes only when the party in power changes; 2) a change in government can be associated with topic changes even if the party in power does not change; 3) economic events, such as financial crises, have less significant and persistent effects than other events such as terrorist attacks. **[More here on the nature of the differences.]**

In this paper we apply a topic model to a dataset of larger-scale parliamentary text records from multiple Australian parliaments and then use a Bayesian hierarchical Dirichlet model to examine events. From a data perspective we contribute **[TBD]**. From a methods perspective, we contribute **[TBD]**. Our model can be used in many settings where the results of topic models need to be further analysed.

Our work is cross-disciplinary and in the traditions of political economy, economic history, political science and statistics. It contributes to a growing modern quantitative social sciences literature that considers text as an input to more traditional methods, rather than requiring separate analysis. As the digitisation of historical sources continues and computational power becomes cheaper, we expect interest in this approach to only increase.


<!-- What events changed the course of topics in Parliament, and what was the pattern before? Eg What did we miss out on because of 9/11, or an assassination? Combine them with the average length that it takes to get legislation through to be specific about what stopped. Similarly, how does the topics change after a change of government? Eg What could have happened if Kevin 07 didn’t kick Howard out. -->


# Data
## Parliamentary text data
Following the example of the UK a daily text record called Hansard of what was said in Australian parliaments has been made available since their establishment.[^Verbatimfootnote] Earlier work on the influence of parliaments, such as @ZandenBuringhBosker2012, often examined broader activity measures such as counts of sitting days. This allowed for long time frames and wide comparisons. But analysing Hansard records and their equivalents directly are an increasingly viable and popular source of data as new methods and reduced computational costs make larger-scale analysis easier, complementing other measures. 

The recent digitisation of the Canadian Hansard, @BeelenEtc2017, allowed @RheaultCochran2018 to examine ideology and party polarisation in Britain and Canada. In the UK, @Duthie2016 analysed Hansard records to examine which politicians made supportive or aggressive statements toward other politicians between 1979 and 1990 and @PetersonSpirling2018 examined polarisation. And as digitisation methods improve older UK records can be analysed, for instance @Dimitruk2018 considers the effect of estate bills on prorogations in seventeenth century England. In New Zealand, @Curran2017 modelled the topics discussed between 2003 and 2016, and @Graham2016 examined unparliamentary language between 1890 and 1950. And in the US @GentzkowShapiroTaddy2018 examine congressional speech records from 1873 to 2016 to find that partisanship has risen in the past few decades.

[^Verbatimfootnote]: While Hansard is not necessarily verbatim, it is considered close enough for text-as-data purposes. For instance, @Mollin2008 found that in the case of the UK Hansard the differences would only affect specialised linguistic analysis. @Edwards2016 examined Australia, New Zealand and the UK, and found that changes were usually made by those responsible for creating the Hansard record, instead of the parliamentarians.

Australian Hansard records have been analysed for various purposes. For instance, @Rasiah2010 examined Hansard records for the Australian House of Representatives to examine whether politicians attempted to evade questions about Iraq during February and March 2003. And @GansLeigh2012 examined Australian Hansard records to associate mentions by politicians of certain public intellectuals with neutral or positive sentiment.

Australian parliaments generally make their daily Hansard records available online as PDFs and these are considered the official release. Additionally, XML records are available in some cases.[^XMLfootnote] We detail the sources of our Hansard PDFs in \@ref(hansardsources) and Appendix \@ref(examplehansardpage) provides an example of what a Hansard PDF looks like. There are roughly 54,331 **[UPDATE]** days worth of publicly available Hansard records across the state and federal parliaments (Table \ref{tab:yearsusedtable}). Our data cleaning process indicates concerns with a small number of PDFs and these are detailed in Appendix \@ref(knownhansardissues).

[^XMLfootnote]: Tim Sherratt makes these XML records available as a single download and also presents them in a website (http://historichansard.net/) that can be used to explore Commonwealth Hansard records from 1901 to 1980. Commonwealth XML records from 1998 to 2014 are available from Andrew Turpin’s website, and from 2006 through to today from Open Australia’s website. The records can also be downloaded from the Australian Hansard website.

```{r yearsusedtable, echo = FALSE, results = 'asis'}

yearsUsed %>%
  kable(
    booktabs = T,
    align = c('l', 'r', 'r', 'r'),
    caption = "Hansard records used") %>%
  kable_styling(font_size = 12)

```

The formatting of the Hansard records changes between the different parliaments and over time. We use scripts written in R (@R2018) to convert the PDFs into daily text records. An example of the workflow and some reduced-detail scripts are provided in Appendix \@ref(hansardworkflow). Some error is introduced at this stage because many of the records are in a two-column format that need to be separated, and the PDF parsing is not always accurate especially for older records. An example of the latter issue is that 'the' is often parsed as 'thc'. These errors are corrected when they occur at scale and can be identified. 

The percentage of stop-words in each record is reasonably consistent over time. This suggests that there is no significant difference in the quality of the parsing over time. Details of this process are provided in Appendix \@ref(stopwordsgraph). We mainly use Hansard records on a daily basis in this paper. Text is usually pre-processed before topic models are used. The specific steps that we take are to: remove numbers and punctuation; change the words to lower case; and concatenate multiword names titles and phrases, such as new zealand to new_zealand. Then the sentences are deconstructed and each word considered individually. 




# Model
The models that we use in this paper are the Structural Topic Model (STM) as implemented by the \texttt{stm} R package of @RobertsStewartAiroldiRPackage, and Bayesian hierarchical Dirichlet analysis model. In a similar way to @MuellerRauh2018, we use topics as an input to another model, in our case to analyse the effect of various events. In Appendix \@ref(word2vec), we include an alternative approach that follows @Taddy2015 by using word2vec, which more closely uses words, rather than topics, as an input.

The basis of the STM is the Latent Dirichlet Allocation (LDA) model of @Blei2003latent. In this section we provide a brief overview of both the LDA model and the STM. We consider the outputs of the topic model as reduced dimension inputs that can be analysed within another model. We then discuss the Bayesian hierarchical Dirichlet analysis model that we use for this purpose.

## Latent Dirichlet Allocation
Although more- or less-fine levels of analysis are possible, but here we are primarily interested in considering a day's topics. This means that each day's Hansard record needs to be classified by its topics. Sometimes Hansard records includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way, especially over longer time periods. One way to get consistent estimates of the topics discussed in Hansard is to use the LDA method of @Blei2003latent, for instance as implemented by the R \texttt{topicmodels} package of @Grun2011.

The key assumption behind the LDA method is that each day's text, 'a document', in Hansard is made by speakers who decide the topics they would like to talk about in that document, and then choose words, 'terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics, where these collections are defined by probability distributions. The topics are not specified *ex ante*; they are an outcome of the method. In this sense, this approach can be considered unsupervised machine learning. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in each day's Hansard group themselves to define topics.

As applied to Hansard, the LDA method considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the speaker then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. Figures \@ref(fig:topicsoverdocuments) and \@ref(fig:topicsoverterms) in Appendix \@ref(LDAexample) illustrates an example with five topics, two documents, and ten terms.

Following @BleiLafferty2009, @blei2012 and @GriffithsSteyvers2004, the process by which a document is generated is more formally considered to be:

1. There are $1, 2, \dots, k, \dots, K$ topics and the vocabulary consists of $1, 2, \dots, V$ terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the $k$th topic is $\beta_k$. Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter $0<\eta<1$ is used: $\beta_k \sim \mbox{Dirichlet}(\eta)$.[^Dirichletfootnote] Strictly, $\eta$ is actually a vector of hyperparameters, one for each $K$, but in practice they all tend to be the same value.
2. Decide the topics that each document will cover by randomly drawing distributions over the $K$ topics for each of the $1, 2, \dots, d, \dots, D$ documents. The topic distributions for the $d$th document are $\theta_d$, and $\theta_{d,k}$ is the topic distribution for topic $k$ in document $d$. Again, the Dirichlet distribution with the hyperparameter $0<\alpha<1$ is used here because usually a document would only cover a handful of topics: $\theta_d \sim \mbox{Dirichlet}(\alpha)$. Again, strictly $\alpha$ is vector of length $K$ of hyperparameters, but in practice each is usually the same value.
3. If there are $1, 2, \dots, n, \dots, N$ terms in the $d$th document, then to choose the $n$th term, $w_{d, n}$:
    a. Randomly choose a topic for that term $n$, in that document $d$, $z_{d,n}$, from the multinomial distribution over topics in that document, $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$.
    b. Randomly choose a term from the relevant multinomial distribution over the terms for that topic, $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$.

[^Dirichletfootnote]: The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, $\eta=1$, it is equivalent to a uniform distribution. If $\eta<1$, then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as $\eta$ decreases. A hyperparameter is a parameter of a prior distribution.

Given this set-up, the joint distribution for the variables is (@blei2012, p.6):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$

Based on this document generation process the analysis problem, discussed next, is to compute a posterior over $\beta_{1:K}$ and $\theta_{1:D}$, given $w_{1:D, 1:N}$. This is intractable directly, but can be approximated (@GriffithsSteyvers2004 and @blei2012).

After the documents are created, they are all that we have to analyse. The term usage in each document, $w_{1:D, 1:N}$, is observed, but the topics are hidden, or 'latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \@ref(fig:topicsoverdocuments) or \@ref(fig:topicsoverterms). In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (@SteyversGriffiths2006). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (@blei2012, p. 7):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$

<!-- The initial practical step when implementing LDA given a collection of documents is to remove 'stop words'. These are words that are common, but that don't typically help to define topics. There is a common list of stop words such as: "a"; "an"; and "and". However the exact list used depends on research of focus. In the case of Australian Hansard, these are words such as: "australia"; "australian"; and "bill". Punctuation and capitalisation is also typically removed. The documents then need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document. -->

<!-- After the dataset is ready, the R \texttt{topicmodels} package of @Grun2011 can be used to implement LDA and approximate the posterior.  -->

Gibbs sampling or the variational expectation-maximization algorithm can be used to approximate the posterior. A summary of these approaches is provided in Appendix \@ref(LDAposteriorestimation). The choice of the number of topics, *k*, drives the results and must be specified *a priori*. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use cross validation.

One weakness of the LDA method is that it considers a 'bag of words' where the order of those words does not matter (@blei2012). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking. This motivates the use of the Structural Topic Model, described in the next section.


## Structural Topic Model
The distinguishing aspect of the Structural Topic Model (STM) of @RobertsStewartAiroldi2016 is that it considers more than just a document's content when constructing topics. For instance, we generally have some information about the author and the date that a document was created. In the case of Hansard, we know who was speaking and the date they spoke. The STM allows this additional information to affect the construction of topics, though influencing either topical prevalence or topical content. That said, the assumption that there is some document generation process is the same as the LDA method, it is just that this process now includes metadata.

The STM is set-up to most easily include metadata to do with prevalence and content. Prevalence relates to the topic proportions in each document. For instance, we expect that topics related to the reasons for Federation, such as tariffs and trade, should be more prevalent in those earlier years than later. Similarly, we may expect topics to do with terrorism to be more prevalent in recent years. Content relates to the words that make up each topic. For instance, there are changes in the use of language over the period for which we have data, and it would be better for these to not be responsible for defining different topics rather than being part of the same topic. The prevalence meta-data for the $d$th document are in $X_d$, which has one column for each aspect. For instance, if there were 10 documents and each had a date and an author, then $X$ would be $10\times 2$. Similarly, the content meta-data are 

As with LDA, the process assumed to generate the documents is the key aspect as this will be reversed to estimate the topics. The document generation process of @Blei2003latent discussed earlier, is slightly modified by @RobertsStewartAiroldi2016 for the STM:

1. As with LDA, the topic distributions, that is, the proportion of a document dedicated to a topic, for the $d$th document are $\theta_d$, and $\theta$ is a vector with length $D$. In contrast to LDA, this is a drawn from a logistic-normal distribution, parameterised such that the mean of that distribution, $\mu$, is affected by a vector of document covariates, $X_d$ (following @RobertsStewartTingley2018, p.3):
$$\theta_d|X_d\gamma\Sigma \sim \mbox{Logistic Normal}(\mu = X_d\gamma, \Sigma)$$

2. To decide the distribution over terms for each topic, $\beta_{d,k}$, start with some baseline distribution over the terms, $m$. Topic-$k$-specific deviations from this are controlled by $\kappa_k^{(t)}$, deviations due to the document meta-data are controlled by $\kappa_{y_d}^{(c)}$, and the interaction between these two deviations is controlled by  $\kappa_{y_d,k}^{(i)}$:
$$\beta_{d,k}\propto \mbox{exp}\left(m+\kappa^{(t)}_{k} + \kappa_{y_d}^{(c)} + \kappa_{y_{d}k}^{(i)}\right) $$

3. Then if there are $n$ terms in the $d$th document, then to choose the $n$th term, $w_{d,n}$:
    a. Randomly choose a topic for that term from the document-specific multinomial distribution over topics.
    b. Randomly choose a term from the topic-specific multinomial distribution over terms.

We primarily implement the STM on the daily-level parliamentary text data described earlier using the \texttt{stm} R package of @RobertsStewartAiroldiRPackage. We consider both topic prevalence and content to be functions of time. The choice of the number of topics to use in the model is a situation-specific compromise. We use a standard diagnostic approach to decide on 80 **[UPDATE]** topics. More detail on this selection process is available in Appendix \@ref(selecttopicnumber).

Figure \@ref(fig:exampletopics) illustrates an output of the STM. This illustrative example only shows five per cent of data from the House of Representatives between 1901 and 2017. It shows how each day's parliamentary discussion can be allocated to a topic and highlights how the prominence of these topics changes over time.

```{r exampletopics, cache = TRUE, echo=FALSE, fig.cap="Illustrative topic model output, with Topic 17 highlighted", out.width = '100%'}
knitr::include_graphics(here::here("outputs/figures/topics_example.pdf"))
```


## Analysis model

<!-- **[Actually, maybe this whole paragraph not needed?]** There are many ways to consider the effect of events. One is to summarise the topic proportions by each event and then run multinomial tests for whether the groups are different, although this does not take full advantage of the frequency of our data. Another is to consider a variant of multinomial logistic regression, but then events would be constrained to have an effect in certain ways. An autoregressive moving average (ARMA) model could be used to test for structural breaks, but **[something]**. Or splines could be fit with specific knot placements and then cross validation used to compare the RSS with a more general splines model, but **[something]**.  -->


The main output of the STM that is of interest to us is the proportion of each topic appearing in each document. Specifically, we are interested in how the distribution of topics changes in relation to different types of events. But with around 80 **[UPDATE]** topics for each of the roughly 54,331 **[UPDATE]** sitting days across the parliaments and houses the data are still too noisy to easily visualise changes around events. Instead, we formalize a statistical framework which allows us to systematically identify significant changes in topic distributions over time. Specifically, we use a Bayesian hierarchical Dirichlet regression framework which relates proportions of each topic to underlying time trends, changes in goverments and elections. 

Define $\gamma_{tp}$ to be the proportion of topic of topic $p$ on day $t$. We assume that the majority of variation in topics is across sitting periods $s$, where a sitting period is defined as any group of days that are less than one week apart. Thus, the topic proportions on day $t$ are modeled in reference to their membership in a particular sitting period $s$. Firstly, we assume that each set of $\gamma_{t,}$ for each day is a draw from a Dirichlet distribution with mean parameter $\mu_{s[t],}$:

$$
\gamma_{t,} &\sim \mbox{Dirichlet}(\mu_{s[t],})
$$
This distributional assumption accounts for the fact that on any given day, the sum of all proportions in each topic must equal 1. 

The goal of the model is to relate these proportions to the government $g$ of time $t$, and also the days since the most recent election, $e$, while account for underlying time trends. Thus, the mean parameters $\mu_{sp}$ are modeled on the log scale as

$$
\log \mu_{s,p} = \alpha_{g[s],p} + \cdot\alpha_{e[s],d,p}  + \sum_{k=1}^{K} \beta_{p,k} \cdot x_{s,k} + \delta_{s,p}
$$
where:

- $\alpha_{g[s],p}$ is the mean effect for government $g$ (which covers sitting period $s$) and topic $p$
- $\alpha_{e[s],d,p}$ is the effect of election $e$ (which occurs in sitting period $s$) for topic $p$ on day $d$ since the election
- $x_{s,k}$ is the $k$th basis spline in sitting period s
- $\beta_{p,k}$ is a cofficient on the $k$th basis spline. 
- $\delta_{s,p}$ is a structured random effect for each sittiing period and topic. 

The government term $\alpha_{g[s],p}$ assumes there is some underlying mean effect of each government on the topic distribution. We place uninformative priors on each of these parameters:
$$
\alpha_{g[s],p} \sim N(0, 100)
$$

The election term $\alpha_{e[s],d,p}$ assumes there is an initial effect of an election on the topic distribution, which then decays as a function of days since election, $d$. In particular, we model this as an AR(1) in $d$:

$$
\alpha_{e[s],d,p} = \rho_{e[s],p} \cdot \alpha_{e[s],d-1,p}
$$
The value of the initial effect $\alpha_{e[s],0,p}$ and the AR(1) term $\rho$ both have non-informative priors:

$$
\alpha_{e[s],0,p} \sim N(0, 100)\\
\rho_{e[s],p} \sim U(0,1)
$$

We model the underlying time trend in topics using splines regression. The intuition behind this term is to capture the underlying non-linear trend in topic distributions over time, which is caused by large-scale structural changes in the economy, and Australian society and culture. The $x_{s,k}$ for $k = 1,2,\dots K$ are the value of cubic basis splines for sitting period $s$ at knot point $k$. We chose to place knot points every 5 sitting periods as this is the average length of time for a government to sit (**CHECK**). Non-informative priors are placed on the splines coefficients:
$$
\beta_{p,k} \sim N(0, 100)
$$

Finally, the sitting period-specific random effect $\delta_{s,p}$ allows for the topic distributions in some sitting periods to be different than expected based on the government/election effects. This allows us to identify large deviations away from the expected distribution, thus helping to identify the effect of other, non-government and non-election events. The $\delta_{s,p}$'s are modeled as 

$$
\delta_{s,p} \sim N(0, \sigma_{e[s],p}^2)
$$

The variance parameters $\sigma_{e[s],p}^2$ give an indication of the how the variation in topics is changing over election periods. If the estimates of the variance are larger, then there is more variation in the topics discussed within an election period. Non-informative priors are placed on the variance parameters:

$$
\sigma_{e[s],p} \sim U(0,3)
$$


RA STUFF:

The full model is:

\begin{align}
\gamma_{stp} &\sim \mbox{Dirichlet}(\mu_{stp}) (\#eq:distributionforgammas) \\ 
\log \mu_{stp} &= \alpha_{sp} + \delta_{stp} (\#eq:meanforgammas) \\
\alpha_{s1} &= 0 (\#eq:zeroconstraint)\\
\alpha_{sp} &\sim \mbox{Normal}\left(\eta_{g[s]}, \sigma^2_g\right) \mbox{ for } p>1 (\#eq:alphasp) \\
\delta_{s,t,p} &\sim \mbox{Normal}\left(\delta_{s,t-1,p}, \sigma^2_{\delta}\right) (\#eq:deltasp) \\
\sigma^2_{\delta} &\sim \mbox{Uniform}(0, 40). (\#eq:randomwalkvariance)  
\end{align} 

**[Does the variance parameter used in Equation 4 need to be specified too?]**

Equation \@ref(eq:distributionforgammas) describes the proportion of each day within a sitting period given to a particular topic. This is considered as a draw from a Dirichlet distribution parameterised by $\mu_{stp}$. In a sense, $\mu_{stp}$ will be mean topic distributions over whatever time period is of interest. 
<!-- The Dirichlet distribution is the multivariate extension of the Beta distribution, and the parameter controls where the 'weight' of the distribution lies.  -->
Equation \@ref(eq:meanforgammas) describes that distribution parameter, $\mu_{stp}$, as the sum of $\alpha_{sp}$, which are sitting period specific topic distributions and mean the model is linear in time for each topic and sitting period, and $\delta_{stp}$, which is a simple time series that allows for a little variation within a sitting period. These are themselves parameters. The primary aim of the modelling task is to find an appropriate and meaningful functional form for $\mu$. 

Equation \@ref(eq:zeroconstraint) constrains the first intercept to zero for identification purposes. Equation \@ref(eq:alphasp) shows how we consider $\alpha_{sp}$ as a draw from a Normal distribution with a mean that depends on **[SOMETHING]** and a variance that depends on **[SOMETHING ELSE]**. Similarly, Equation \@ref(eq:deltasp) show how we model $\delta_{stp}$  as a draw from a Normal distribution with a mean that depends on the preceding $\delta$ and some variance which is drawn from the Uniform distribution in Equation \@ref(eq:randomwalkvariance), that is, a random walk around each intercept.

We run the model in Stan using the \texttt{RStan} package of @RStan. An illustration of the validity of our analysis model using simulated data is in Appendix \@ref(modeldetailsandsimulation). 


**[Probably need to add: 1) where does each covariate come into it and how do they interact; 2) how does correlation work; 3) what are the random effects and fixed effects?]**





<!-- We currently consider events in two ways. The first is graphically, grouping each of the daily topic proportions by the events and isolating the topics. This illustrates the differences between the topics. The second is by including incrementing variables in the prevalence metadata This groups each day by events and we can then conduct significance tests. Unfortunately both of these do not consider short-term effects. -->


<!-- **[Use tf-idf between them as illustrative instead? Use that other measure of difference?]** -->


<!-- We consider events in two ways. The first is in the change in the word usage before and after events and the second is in changes in the topics.  -->

<!-- Differences in word usage can be evaluated using the term frequency-inverse document frequency (tf-idf) measure. It will be higher for words that are rarely used across all documents, but commonly used in a document. For instance, in Australian parliamentary text records 'the' is commonly used in many documents and so the fact that it is used in any particular document is not usual. However, 'aboriginal' is less common across, and so if it was especially prevalent in a particular document that may distinguish that document. -->

<!-- When we consider events using tf-idf, we gather terms from more than one day. We define groups of days that are roughly analogous to sitting periods. If there is more than a week between a day then we define a new group, otherwise the day is in the existing group. We use various measures **(Text2vec R package has some? or some other package)** to define a baseline measure of how different each group is, and then test for whether groups separated by our events are significantly different to this baseline. -->



# Results
We are interested in considering the effect of various events on what is talked about in Australia's parliaments. 
<!-- Each of the parliaments and their houses are treated independently here. Future work could expand the model to better understand, and allow, for correlation between them.  -->
Political events are those related to a change of government or an election. Other events are defined by substantial changes in various economic measures, such as the onset of the Great Depression or floating the currency; events of a historical magnitude, such as entering into a war or the 9/11 attacks; or events that had a significant effect on Australian life, such as hosting the Olympics, or the Mabo decision. The full list of events that we consider are detailed in Appendix \@ref(eventdetails).

First considering the mean topic distributions over governments, Figures \@ref(fig:mu1to20) - \@ref(fig:mu41to60).

```{r mu1to20, cache = TRUE, echo=FALSE, fig.cap="Illustrative topic model output, with Topic 17 highlighted", out.width = '100%'}
knitr::include_graphics(here::here("outputs/figures/mu_topic1-20.pdf"))
```


```{r mu21to40, cache = TRUE, echo=FALSE, fig.cap="Illustrative topic model output, with Topic 17 highlighted", out.width = '100%'}
knitr::include_graphics(here::here("outputs/figures/mu_topic21-40.pdf"))
```


```{r mu41to60, cache = TRUE, echo=FALSE, fig.cap="Illustrative topic model output, with Topic 17 highlighted", out.width = '100%'}
knitr::include_graphics(here::here("outputs/figures/mu_topic41-60.pdf"))
```

Topics related to Federation such as the British Empire and the constitution decrease in importance over time. A substantial increase in the importance of issues dealing with 

Increasing from 20th gov: 2, 4, 8, 12, 

32 increases and then decreases

Increases only recently: 33, 35

Breakpoint: 23, 25, 49, 48, 46, 47, 41


Figure \@ref(fig:electionsevents) shows the grouping by election. If elections had significant effects on the discussion in parliament then there would be considerable change between groups. However changes generally appear to be longer-term rather than election to election. 

Groupings by government. This can be different to election groupings, for instance, the change from the Rudd to Gillard governments happened without an election. Similarly, John Howard's three terms are all considered the one government for our purposes.

Here we see much more difference between adjacent events. This suggests that new governments tend to talk about different topics than the government they replace.




For instance, all of the incrementing variables have a significant effect on the prevalence of Topic 1. Table \@ref(tab:topwordssig) shows that this has to do with tariffs and trade. On the other hand, Topic 2 has to do with aspects of daily life such as community and children, and Topic 3 has to do with legal issues and neither is significant. This is not surprising given the centrality of these concerns at all times.

```{r topwordssig, echo = FALSE, results = 'asis'}

top_words %>% 
  kable(booktabs = T, caption = "Top words for each topic") %>%
  kable_styling(font_size = 8)

```



# Summary and conclusions
In this paper we examined what was said in Australia's parliaments. We downloaded and parsed PDFs for Australian states and federal parliaments. We then used a text model to group the discussions into topics and analysed the effect of various events on the distribution of the discussion. 

In general we found that changes in government changed the distribution of topics discussed in parliament, but that elections did not. We found that significant events such as 9/11 had substantial and lasting changes, but that with certain exceptions, economic events did not.

Text analysis has well-known biases and weaknesses and is a complement to more detailed analysis such as qualitative methods and case studies. We consider the results presented in this paper, as well as many of those results of the larger text-as-data research program, as fitting within findings based on other methods.

While using text as data has well-known shortcomings, it allows larger-scale analysis that would not be viable using less-automated approaches and so it can identify patterns that may otherwise be overlooked.

<!-- What could happen if we had longer terms. Eg GST needed multiple generations of politicians but carbon tax couldn’t because it was one generation. -->




\newpage

# (APPENDIX) Appendix {-}

# Hansard details

## Hansard sources {#hansardsources}

Hansard records are available from a variety of sources, as detailed in Table \ref{tab:hansardsourcestable}.


```{r hansardsourcestable, cache = TRUE, echo = FALSE, results = 'asis'}

hansard_sources %>%
  kable(
    booktabs = T,
    align = c('l', 'r', 'r', 'r'),
    caption = "Hansard sources and notes") %>%
  kable_styling(font_size = 10)

# Break cachee

```

Detail where from, and which years are being used and why.

Which years are being used (not non-OCRd)


## Example Hansard page {#examplehansardpage}

Figure \@ref(fig:asdf)

```{r asdf, echo=FALSE, fig.cap="Example Hansard page - 6 February 1902", out.width = '100%'}
knitr::include_graphics(here::here("outputs/figures/example_hansard_page.png"))
```

## Known Hansard issues {#knownhansardissues}

The NSW Legislative Council was established earlier than 1856, however the earlier Hansard records have not been through an independent OCR process and were not used in this paper. However, the Google Tesseract OCR engine as implemented by @Ooms2018tesseract provided useful data and these could be used in the future.



Which PDFs are missing or have no content, etc.


```{r missingdays, echo = FALSE, results = 'asis'}

missing_days %>% 
  kable(booktabs = T, caption = "Missing days") %>%
  kable_styling(font_size = 12)

```


Notes: Positive means I am missing some, Negative means I have too many.
Source: https://www.aph.gov.au/Parliamentary_Business/Statistics/Senate_StatsNet/General/sittingdaysyear 



## Example Hansard PDf to text record workflow {#hansardworkflow}
Example of the workflow from PDF to text

These scripts are primarily based on: the \texttt{PDFtools} package of @Ooms2018pdftools; the \texttt{tidyverse} package of @Wickham2017; the \texttt{tm} package of @FeinererHornik2018; the \texttt{lubridate} package of @GrolemundWickham2011; and the \texttt{stringi} package of @Gagolewski2018. 

The functions of those packages are supported by: the \texttt{furrr} package of @VaughanDancho2018; and the \texttt{tictoc} package of @Izrailev2014. 

The \texttt{hunspell} package of @Ooms2017 is also used to help find spelling issues. 

In addition to the packages already mentioned, in this step the R scripts to do this use the \texttt{tidytext} R package of @SilgeRobinson2016. 

And @Benoit2018 for compounding multiword expressions.


## Stopwords over time {#stopwordsgraph}
Insert graph of stop words over time.


\newpage





# word2vec alternative {#word2vec}
An alternative approach that follows @Taddy2015. 



\newpage



# Topic modelling example and details
## Examples {#LDAexample}
For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \@ref(fig:topicsoverdocuments)).

```{r topicsoverdocuments, cache = TRUE, echo=FALSE, fig.cap = "Probability distributions over topics for two documents", fig.height = 4}

tibble(
  Topics = rep(c("1", "2", "3", "4", "5"), 2),
  Probability = c(c(0.40, 0.40, 0.1, 0.05, 0.05), c(0.01, 0.04, 0.35, 0.20, 0.4)),
  Document = c(rep("Document 1", 5), rep("Document 2", 5))
  ) %>%
  ggplot(aes(Topics, Probability, colour = Document)) +
  geom_point() +
  theme_classic() +
  coord_flip()  +
  scale_colour_viridis_d()

```


For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \@ref(fig:topicsoverterms)).


```{r topicsoverterms, cache = TRUE, echo=FALSE, fig.cap = "Probability distributions over terms", fig.height = 4}
tibble(
  Terms = rep(c("migration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"), 2),
  Probability = c(c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2), c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)),
  Topics = c(rep("Topic 1", 10), rep("Topic 2", 10))
  ) %>%
  ggplot(aes(Terms, Probability, colour = Topics)) +
  geom_point() +
  theme_classic() +
  coord_flip() +
  scale_colour_viridis_d()

```



## Posterior estimation {#LDAposteriorestimation}
Following @SteyversGriffiths2006 and @Darling2011, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with $\alpha = \frac{50}{K}$ and $\eta = 0.1$  (@SteyversGriffiths2006 recommends $\eta = 0.01$), where $\alpha$ refers to the distribution over topics and $\eta$ refers to the distribution over terms (@Grun2011, p. 7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (@Grun2011, p. 6):
$$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$
where $z'_{d, n}$ refers to all other topic assignments; $\lambda'_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$; $\lambda'_{.\rightarrow k}$ is a count of how many other times that any term has been assigned to topic $k$; $\lambda'^{(d)}_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$ in that particular document; and $\lambda'^{(d)}_{-i}$ is a count of how many other times that term has been assigned in that document. Once $z_{d,n}$ has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (@SteyversGriffiths2006). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.


## Selection of number of topics {#selecttopicnumber}

**[TBD]**


## Robustness of results
**[IS THIS BEING USED?]**
Here we change the number of sitting days considered either side of an event. The results in the main section of the paper are for the nearest ten days either side of an event. Here are show that the results are essentially the same if the nearest one, two, five, and twenty days either side of an event.



\newpage


# Events {#eventdetails}



```{r governments, echo = FALSE, results = 'asis'}

governments %>% 
  kable(booktabs = T, caption = "Change in governments") %>%
  kable_styling(font_size = 10)

```


```{r elections, echo = FALSE, results = 'asis'}

elections %>% 
  filter(election == 1) %>% 
  select(year, electionDate, electionWinner) %>% 
  kable("latex", booktabs = T, caption = "Elections") %>%
  kable_styling(font_size = 10)

```


```{r keyevents, echo = FALSE, results = 'asis'}

keyEvents %>% 
  kable(booktabs = T, caption = "Key events") %>%
  kable_styling(font_size = 10)

```


For background on the Premiers' Plan see @Copland1934.

For background on the Gruen tariff cut see @Gruen1975.

Add the graphs and procedures.


\newpage


# Analysis model {#modeldetailsandsimulation}


Here we illustrate the validity of our analysis model using simulated data.

\newpage


# References

