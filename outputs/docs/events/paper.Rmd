---
output:
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    number_sections: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
title: "The Effect of Elections and Changed Governments on Discussion in the Australian Federal Parliament (1901--2017)"
thanks: "Thank you to Chris Cochrane, Dan Simpson, Jill Sheppard, John Tang, Leslie Root, Martine Mariotti, Matt Jacob, Matthew Kerby, Tianyi Wang, Tim Hatton, and Zach Ward for their helpful suggestions; and to the UC Berkeley Demography Department for the use of their computing resources. We are grateful for the many excellent comments that we received from seminar participants at the ANU School of Politics and International Relations, the ANU Research School of Economics, and the Parliamentary Library. Comments and suggestions on the `r format(Sys.time(), '%d %B %Y')` version of this paper welcome at: rohan.alexander@anu.edu.au."
author:
- name: Monica Alexander
  affiliation: University of Toronto
- name: Rohan Alexander
  affiliation: Australian National University
abstract: "We systematically analyse how parliamentary discussion changes in response to elections and changed governments in Australian history. We first create a dataset of what was said in the Australian Federal Parliament from 1901 through to 2017 based on available public records. To reduce the dimensionality of this dataset we use a correlated topic model, and then analyse the effect of various events using a Bayesian hierarchical Dirichlet model. We find that: changes in government tend be associated with topic changes even when the party in power does not change; elections that do not result in a change in government are rarely associated with topic changes; economic events, such as financial crises, have less significant effects than other events such as terrorist attacks; and the effect of events is much more pronounced in the second half of our sample, and especially in the past two decades. Our findings have implications for how we think about the longer-term trajectory of government policymaking as the media and political cycles becomes increasingly focused on short-term events."
keywords: "text, politics, Australia, unsupervised machine learning, Bayesian model."
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
toc: FALSE
# spacing: double
bibliography: ../bibliography.bib
biblio-style: apsr
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(kableExtra)
library(knitr)
library(tidyverse)

elections <- read_csv(here::here("inputs/misc/misc_elections_data.csv"), col_types = cols())
governments <- read_csv(here::here("inputs/misc/change_of_pm.csv"), col_types = cols())
keyEvents <- read_csv(here::here("inputs/misc/key_events.csv"), col_types = cols())
significances <- read_csv(here::here("outputs/misc/significance.csv"), col_types = cols())
top_words <- read_csv2(here::here("outputs/results-topic_models_and_gammas/top_words_80.csv"), col_types = cols())

top_words <- top_words %>% 
  rename(Topic = topic, Terms = terms)

hansard_sources <- read_csv(here::here("outputs/misc/hansard_sources.csv"), col_types = cols())

elections_results <- read_csv(here::here("outputs/results-analysis_model/results-elections.csv"), col_types = cols()) %>% 
  # filter(Significant %in% c("Yes", "No"))
  filter(Significant %in% c("Yes"))

governments_results <- read_csv(here::here("outputs/results-analysis_model/results-governments.csv"), col_types = cols()) %>% 
  filter(Significant %in% c("Yes")) %>% 
  # filter(Significant %in% c("Yes", "No")) %>% 
  select(-Party, -`Died in Office`)

key_events_results <- read_csv(here::here("outputs/results-analysis_model/results-outliers.csv"), col_types = cols())

```


# Introduction
Government policy is partly driven by parliamentary discussion. Conversely, that same discussion can indicate a government's priorities. But major events -- be they expected, such as an election, and unexpected, such as a recession or terrorist attack -- can affect the course of parliamentary discussion. For instance, think of how a new government often goes to some trouble to seem different to the one they replace, or how events such as the 9/11 attacks altered government priorities.

In this paper we examine text records of what was said in the Australian Federal Parliament. We create a dataset based on all sitting days from 1901 through to 2017. We have records for 7,869 days in the House of Representatives and 6,682 days in the Senate. Our dataset consists of counts of every word that occurs at least thrice, after we convert all words to lower-case, remove numbers and punctuation, join commonly co-located words, and remove stop words.

We systematically analyse the text using two main statistical techniques. To reduce the dimensionality of our dataset we first use a topic model that allows correlation between topics. We then analyse those topics using a Bayesian hierarchical Dirichlet model to examine changes at various types of events. These events include: changes in government; elections; and other significant events such as an economic recession or terrorism.

We find: 1) changes in government tend be associated with topic changes even when the party in power does not change. For instance, the change from Hughes to Bruce in 1922, Menzies to Holt in 1966; Hawke to Keating in 1991; and Rudd to Gillard in 2010 are all associated with significant changes in topics despite no change in the party in power. 2) As expected, elections where the party in power also changes, such as Fisher in 1910 and 1914, Menzies in 1949, Whitlam in 1972, Fraser in 1975, Hawke in 1983, Howard in 1996, Rudd in 2007, and Abbott in 2013 are associated with topic changes, but the 1974 Whitlam, 1980 Fraser, and the 1998 and 2004 Howard re-elections, stand out as elections where the government was returned yet there was a significant change in topics. 3) Economic events, such as financial crises, have less significant effects than other events such as terrorism.

As our dataset covers 117 years we are able to see how the effect of events changes over time. We find that the events that we are interested in are rarely significant in the first half of our dataset. With a small number of exceptions, even changes of government where the party in power also changed were not associated with overly large changes in parliamentary discussion. If governments try to more thoroughly distinguish themselves from their predecessor then longer-term policy may be more difficult to enact. Similarly, there is the danger that larger-scale signature-accomplishments of a government may be neglected by their successor for solely political reasons.

Our work contributes to a growing modern quantitative social sciences literature that considers text as an input to more traditional methods, rather than requiring separate analysis. This literature sits across and draws from various historically-separate disciplines including economic history, political science, and applied statistics. We contribute to this literature in terms of both data and methods. From a data perspective we bring to bear an essentially-complete record of what was said in the Australian Federal Parliament, and we make our dataset available to other researchers via the R package \texttt{hansardr}. From a methods perspective, our analysis model has several advantages over existing methods. These include: allowing the events to have more-complicated auto-correlated functional forms; implementing pooling across groups of similar documents; and identifying outlying topic distributions without the need to pre-specify the event of interest. As the digitisation of historical sources continues and computational power becomes cheaper, we expect interest in this approach to only increase.





<!-- Selling points: -->
<!-- Almost complete record. Unique word count. -->
<!-- Different events method. -->
<!-- Highlight increase in volatility. ADD TO ABSTRACT. DISCUSS IMPLICATIONS IN DISCUSSION. Personality driven. -->
<!-- Gary King paper? -->





<!-- What events changed the course of topics in Parliament, and what was the pattern before? Eg What did we miss out on because of 9/11, or an assassination? Combine them with the average length that it takes to get legislation through to be specific about what stopped. Similarly, how does the topics change after a change of government? Eg What could have happened if Kevin 07 didnâ€™t kick Howard out. -->


# Data

Following the example of the UK a daily text record called Hansard of what was said in the Australian Federal Parliament has been made available since it was established in 1901.[^Verbatimfootnote] Earlier work on the influence of parliaments, such as @ZandenBuringhBosker2012, often examined broader activity measures such as counts of sitting days. This allowed for long time-frames and wide comparisons. But analysing Hansard records and their equivalents directly is increasingly viable as new methods and reduced computational costs make it easier.

[^Verbatimfootnote]: While Hansard is not necessarily verbatim, it is considered close enough for text-as-data purposes. For instance, @Mollin2008 found that in the case of the UK Hansard the differences would only affect specialised linguistic analysis. @Edwards2016 examined Australia, New Zealand and the UK, and found that changes were usually made by those responsible for creating the Hansard record, instead of the parliamentarians. As those who create Hansard are tasked with creating an accurate record of proceedings, this suggests the records should be fit for the purpose of our analysis.

The recent digitisation of the Canadian Hansard, @BeelenEtc2017, allowed @RheaultCochran2018 to examine ideology and party polarisation in Britain and Canada. In the UK, @Duthie2016 analysed Hansard records to examine which politicians made supportive or aggressive statements toward other politicians between 1979 and 1990 and @PetersonSpirling2018 examined polarisation. And as digitisation methods improve older UK records can be analysed, for instance @Dimitruk2018 considers the effect of estate bills on prorogations in seventeenth century England. In New Zealand, @Curran2017 modelled the topics discussed between 2003 and 2016, and @Graham2016 examined unparliamentary language between 1890 and 1950. And in the US, @GentzkowShapiroTaddy2018 examined congressional speech records from 1873 to 2016 to find that partisanship has risen in the past few decades.

Parts of Australian Hansard records have been analysed for various purposes. For instance, @Rasiah2010 examined Hansard records for the Australian House of Representatives to examine whether politicians attempted to evade questions about Iraq during February and March 2003. @GansLeigh2012 examined Australian Hansard records to associate mentions by politicians of certain public intellectuals with neutral or positive sentiment. And @Salisbury2011 examines unparliamentary behaviour. @FraussenGrahamHalpin2019 examined Australian Hansard records to assess the prominence of interest groups. The closest research to ours that we have found is @Boulous2013 who examines parliamentary debate in Australia for the period 1946 to 2012.

The Australian Federal Parliament makes daily Hansard records available online as PDFs and these are considered the official release. Additionally, XML records are available in some cases.[^XMLfootnote] We provide an example of a Hansard PDF page in Appendix \@ref(examplehansardpage). There are 14,551 days of publicly available Hansard records across both chambers of the Australian Federal Parliament that we have PDFs for and further summary statistics for this are provided in Appendix \@ref(hansardsummarystatistics). Our data cleaning process indicates concerns with a small number of PDFs and these are detailed in Appendix \@ref(knownhansardissues).

[^XMLfootnote]: Tim Sherratt makes these XML records available as a single download and also presents them in a website (http://historichansard.net/) that can be used to explore Commonwealth Hansard records from 1901 to 1980. Commonwealth XML records from 1998 to 2014 are available from Andrew Turpinâ€™s website, and from 2006 through to today from Open Australiaâ€™s website. The records can also be downloaded from the Australian Hansard website.

We use the official PDF release and the formatting of the Hansard records changes over time. We use scripts written in R (@R2018) to convert the PDFs into daily text records. An example of the workflow and some reduced-detail scripts are provided in Appendix \@ref(hansardworkflow). Some error is introduced at this stage because many of the records are in a two-column format that need to be separated, and the PDF parsing is not always accurate especially for older records. An example of the latter issue is that 'the' is often parsed as 'thc'. These errors are corrected when they occur more than twice and can be identified.

The percentage of stop-words in each record is reasonably consistent over time. This suggests that there is no significant difference in the quality of the parsing over time. Details of this check are provided in Appendix \@ref(stopwordsgraph). We use Hansard records on a daily basis in this paper. We pre-process our text before applying a topic model. The specific steps that we take are to: remove numbers and punctuation; change the words to lower case; and concatenate multi-word names titles and phrases, such as new zealand to new_zealand. Then the sentences are de-constructed and each word considered individually.



# Model

The goal of our modelling strategy is twofold. Firstly, we want to use topic modelling [@Blei2003latent] to summarise the Hansard text into meaningful topics that reduce the dimensionality of the text data and capture the main themes discussed in parliament over time. Secondly, we want to relate the resulting topic distributions to temporal trends, changes, and events, such as a change in government, elections, and other external events. There are two stages in our analytical process.

We first use the Correlated Topic Model (CTM) [@BleiLafferty2007] to obtain estimated topic distributions over time. We consider these topic distributions as reduced dimension inputs that can be analysed within another model. We then formulate a Bayesian hierarchical Dirichlet model to assess changes in the topic distributions in relation to events of interest. 

In the following section, we briefly describe the topic modelling approach, which is to use the CTM, which is a natural extension of the Latent Dirichlet Allocation (LDA) model [@Blei2003latent], before discussing the Bayesian hierarchical Dirichlet analysis model used to investigate changes in topics. More detail on the topic modelling is available for readers that may be less familiar with it in Appendix \@ref(LDAexample).

<!-- The models that we use in this paper are the Structural Topic Model (STM) as implemented by the \texttt{stm} R package of @RobertsStewartAiroldiRPackage, and Bayesian hierarchical Dirichlet analysis model. In a similar way to @MuellerRauh2018, we use topics as an input to another model, in our case to analyse the effect of various events.  -->

<!-- The basis of the STM is the Latent Dirichlet Allocation (LDA) model of @Blei2003latent. In this section we provide a brief overview of both the LDA model and the STM. We consider the outputs of the topic model as reduced dimension inputs that can be analysed within another model. We then discuss the Bayesian hierarchical Dirichlet analysis model that we use for this purpose. -->

## Latent Dirichlet Allocation
Although more- or less-fine levels of analysis are possible, here we are primarily interested in considering a day's topics. This means that each day's Hansard record needs to be classified by its topics. Sometimes Hansard records includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way, especially over longer time periods. 

Other work such as @BaumgartnerJones1993 and @DowdingHindmoor2010 does this by creating a standardised codebook of policy categories and sub-categories and then manually assigning text to topics as appropriate. This approach ensures the categorisation is reasonable but as it is a manual process the size of the text that can be categorised is limited. In exchange for some reduction in the reasonableness of the categorisation, the LDA method of @Blei2003latent is able to provide consistent categorisation of the topics discussed in Hansard for large text collections.

The key assumption behind LDA is that each day's text, 'a document', in Hansard is made by speakers who decide the topics they would like to talk about in that document, and then choose words, 'terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics, where these collections are defined by probability distributions. The topics are not specified *ex ante*; they are an outcome of the method. In this sense, this approach can be considered unsupervised machine learning. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. The goal is to have the words found in each day's Hansard group themselves to define topics. This can provide more flexibility than other approaches such as a strict word count method, but can require a larger dataset and make interpretation more difficult. More detail on how LDA works is available in Appendix \@ref(LDAexample).

One weakness of the LDA method is that it considers a 'bag of words' where the order of those words does not matter [@blei2012]. It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. This is the Correlated Topic Model, described in the next section.


## Correlated Topic Model
As mentioned in the previous section, one of the limitations of LDA is that the model assumes that the presence of one topic is not correlated with the presence of another topic. Correlation between topics is neither modelled nor accounted for by LDA, but in reality often topics are related. For instance, in the Hansard context, we may expect topics related to the army to be more commonly found with topics related to the navy, but less commonly with topics related to banking. The goal of the CTM [@BleiLafferty2007] is to account for this correlation between topics, in order to produce more realistic and stable topic distributions over time. The models are very similar, and the key difference is the underlying distributions that are drawn from.

As with LDA, the process assumed to generate the documents is the key aspect as this will be reversed to estimate the topics. The document generation process of @Blei2003latent discussed earlier, is just slightly modified. Rather than assuming that the distribution of topics in a document, $\theta_d$, are a draw from a Dirichlet distribution, as in step 2 in LDA above, CTM assumes

$$\theta_d \sim \mbox{Logistic Normal}(\mu, \Sigma)$$
That is, the main difference of CTM over LDA is that it replaces the assumption of the Dirichlet distribution with a more flexible logistic multivariate Normal distribution. This distribution can incorporate a covariance structure across the topics. The remainder of the steps of the document generating process are identical to LDA. 

However, the replacement of the Dirichlet distribution with the logistic multivariate Normal distribution adds a level of computational complexity to CTM. The posterior distributions of the parameters of interest ($\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}$) can no longer be obtained using standard simulation techniques such as Gibbs Sampling. @BleiLafferty2007 develop a fast variational inference procedure for estimating the CTM. CTM itself has also been extended by @RobertsStewartAiroldi2016 as part of their work on Structural Topic Models. The main difference is to add a covariate to $\mu$ which allows consideration of additional information.


<!-- The distinguishing aspect of the Structural Topic Model (STM) of @RobertsStewartAiroldi2016 is that it considers more than just a document's content when constructing topics. For instance, we generally have some information about the author and the date that a document was created. In the case of Hansard, we know who was speaking and the date they spoke. The STM allows this additional information to affect the construction of topics, though influencing either topical prevalence or topical content. That said, the assumption that there is some document generation process is the same as the LDA method, it is just that this process now includes metadata. -->

<!-- The STM is set-up to include metadata to do with prevalence and content. Prevalence relates to the topic proportions in each document. For instance, we expect that topics related to the reasons for Federation, such as tariffs and trade, should be more prevalent in those earlier years than later. Similarly, we may expect topics to do with terrorism to be more prevalent in recent years. The prevalence meta-data for the $d$th document are in $X_d$, which has one column for each covariate. For instance, if there were 10 documents and each had a date and an author, then $X$ would be $10\times 2$. Content relates to the words that make up each topic. For instance, there are changes in the use of language over the period for which we have data, and it would be better for these to not be responsible for defining different topics rather than being part of the same topic. STM only allows for one covariate related to content changes, for example, documents could be grouped by year.  -->

<!-- As with LDA, the process assumed to generate the documents is the key aspect as this will be reversed to estimate the topics. The document generation process of @Blei2003latent discussed earlier, is slightly modified by @RobertsStewartAiroldi2016 for the STM: -->

<!-- 1. As with LDA, the topic distributions, that is, the proportion of a document dedicated to a topic, for the $d$th document are $\theta_d$, and $\theta$ is a vector with length $D$. In contrast to LDA, this is a drawn from a logistic-normal distribution, parameterised such that the mean of that distribution, $\mu$, is affected by a vector of document covariates, $X_d$ (following @RobertsStewartTingley2018, p.3): -->
<!-- $$\theta_d|X_d\gamma\Sigma \sim \mbox{Logistic Normal}(\mu = X_d\gamma, \Sigma)$$ -->

<!-- 2. To decide the distribution over terms for each topic, $\beta_{d,k}$, start with some baseline distribution over the terms, $m$. Topic-$k$-specific deviations from this are controlled by $\kappa_k^{(t)}$, deviations due to the document meta-data are controlled by $\kappa_{y_d}^{(c)}$, and the interaction between these two deviations is controlled by  $\kappa_{y_d,k}^{(i)}$: -->
<!-- $$\beta_{d,k}\propto \mbox{exp}\left(m+\kappa^{(t)}_{k} + \kappa_{y_d}^{(c)} + \kappa_{y_{d}k}^{(i)}\right) $$ -->

<!-- 3. Then if there are $n$ terms in the $d$th document, then to choose the $n$th term, $w_{d,n}$: -->
<!--     a. Randomly choose a topic for that term from the document-specific multinomial distribution over topics. -->
<!--     b. Randomly choose a term from the topic-specific multinomial distribution over terms. -->

<!-- We primarily implement the STM on the daily-level parliamentary text data described earlier using the \texttt{stm} R package of @RobertsStewartAiroldiRPackage. We consider both topic prevalence and content to be functions of time. The choice of the number of topics to use in the model is a situation-specific compromise. We use a standard diagnostic approach to decide on 80 **[UPDATE]** topics. More detail on this selection process is available in Appendix \@ref(selecttopicnumber). -->

Figure \@ref(fig:exampletopics) illustrates a CTM output based on a five per cent sample from the Australian House of Representatives between 1901 and 2017. It shows how each day's parliamentary discussion can be apportioned to a topic and highlights how these proportions change over time.

```{r exampletopics, cache = TRUE, echo=FALSE, fig.cap="Illustrative topic model output, with Topic 17 highlighted", out.width = '100%'}
# knitr::include_graphics("../../figures/topics_example.pdf")
```


## Analysis model

<!-- **[Actually, maybe this whole paragraph not needed?]** There are many ways to consider the effect of events. One is to summarise the topic proportions by each event and then run multinomial tests for whether the groups are different, although this does not take full advantage of the frequency of our data. Another is to consider a variant of multinomial logistic regression, but then events would be constrained to have an effect in certain ways. An autoregressive moving average (ARMA) model could be used to test for structural breaks, but **[something]**. Or splines could be fit with specific knot placements and then cross validation used to compare the RSS with a more general splines model, but **[something]**.  -->


The CTM output of interest is the proportion of each topic appearing in each document, that is, the $\theta_d$. The aim of this stage of the modelling process is to analyse how the distribution of topics changes in relation to different types of events. But with 80 topics for each of the roughly 14,551 chamber-sitting-days the data are still too noisy to easily visualise changes around events.

One option for relating the topic distributions to events would be to use the Structural Topic Model (STM) of @RobertsStewartAiroldi2016. The distinguishing aspect of the STM is that it considers more than just a document's content when constructing topics. For example, we may believe a document's author, or the time at which it was written, or, in the case of Hansard, the government or election period, may affect the topics within that document. The STM allows this additional information, or metadata, to affect the construction of topics, though influencing either topical prevalence or topical content. The assumption that there is some document generation process is the same as in LDA, it is just that this process now includes metadata.

More specifically, consider a matrix of covariates. $X_d$, where each column relates to a different covariate or metadata aspect, and each row refers to a document. Then a cell has the value of each covariate for a particular document. Similar to the CTM, the STM assumes the topics within a document $\theta_d$ are a draw from a logistic Normal distribution with mean $\mu$. However, the STM framework assumes that the mean parameter is a linear function of covariates $X_d$:

$$\theta_d|X_d\gamma\Sigma \sim \mbox{Logistic Normal}(\mu = X_d\gamma, \Sigma)$$

Using the STM framework with covariates could theoretically allow the relationships between topics and certain types of events to be assessed. For instance, one covariate could be the government $g$ that was in power during the time corresponding to document $d$. However, the STM covariate framework has several limitations in terms of our goal to assess the relationship between topics and events:

- There is no way of specifying more complicated auto-correlated functional forms of the effects of events over time. For example, we believe that the effect of an election would peak at the time of the election, then gradually decay as a function of days since election. In the STM framework, it is possible to specify a constant or linear effect of elections over time, or a spline relationship over elections, but it is not possible to restrict the effect of a specific election over time to be monotonically decreasing. 
- There is no way to implement partial pooling across groups of similar documents. The STM framework assumes that  documents are independently and identically distributed, conditional on the model covariates. However, it could be expected that topic distributions within a particular government, for example, may be more- or less-likely to contain certain topics for reasons that are not reflected in the topic prevalence covariates. To account for this, we would like a covariate model that allows for the partial pooling of variance in topic distributions by group, such as sitting period. 
- There is no way of identifying 'outlying' topic distributions -- and therefore events that had an important effect -- without pre-specifying the event of interest in the model. For example, if we think that the 9/11 attacks had an effect on parliamentary discourse, then a dummy for 9/11 would have to be included in the STM framework, but the specifics of the dummy construction affect the results. Ideally, we would like to identify important events based on different-to-expected topic distributions, after accounting for time trends, government and election effects. 

To overcome these challenges, we formalise a statistical framework that allows us to systematically identify significant changes in topic distributions over time. Specifically, we use the estimated topic distributions from the CTM described in the previous section as an input into a Bayesian hierarchical Dirichlet regression framework, which relates the proportions of each topic to underlying time trends, changes in governments and elections. This set-up also allows us to identify 'outlying' topic distributions and relate these to other events. 

Define $\theta_{dp}$ to be the proportion of topic of topic $p$ on day $d$. Note that the $\theta_{d,1:P}$ for $p = 1,2,\dots P=40$ are equal to the estimated values of $\theta_d$ from the CTM. We assume that the majority of variation in topics is across sitting periods $s$, where a sitting period is defined as any group of days that are less than one week apart. Using this definition, there are a total of 745 sitting periods over the period 1901 to 2017 inclusive.

The topic proportions on day $d$ are modelled in reference to their membership of a particular sitting period $s$. Firstly, we assume that each distribution of topics, $\theta_{d,1:P}$ for each day is a draw from a Dirichlet distribution with mean parameter $\mu_{s[d],1:P}$:

$$
\theta_{d,1:P} \sim \mbox{Dirichlet}(\mu_{s[d],1:P})
$$
where the notation $s[d]$ refers to the sitting period $s$ to which day $d$ belongs. This distributional assumption accounts for the fact that on any given day, the sum of all proportions in each topic must equal 1. 

The goal of the model is to relate these proportions to government $g$ at time $d$, and also the days since the most recent election, $e$, while account for underlying time trends. The mean parameters $\mu_{s,p}$ are modelled on the log scale as

$$
\log \mu_{s,p} = \alpha_{g[s],p} + \cdot\alpha_{e[s],d,p}  + \sum_{k=1}^{K} \beta_{p,k} \cdot x_{s,k} + \delta_{s,p}
$$
where:

- $\alpha_{g[s],p}$ is the mean effect for government $g$ (which covers sitting period $s$) and topic $p$;
- $\alpha_{e[s],d,p}$ is the effect of election $e$ (which occurs in sitting period $s$) for topic $p$ on day $d$ since the election;
- $\sum_{k=1}^{K} \beta_{p,k} \cdot x_{s,k}$ is the underlying time trend, modelled using splines: $x_{s,k}$ is the $k$th basis spline in sitting period $s$ and $\beta_{p,k}$ is a coefficient on the $k$th basis spline; and
- $\delta_{s,p}$ is a structured random, or levels, effect for each sitting period and topic. 

The government term $\alpha_{g[s],p}$ assumes there is some underlying mean effect of each government on the topic distribution. We place uninformative priors on each of these parameters:
$$
\alpha_{g[s],p} \sim \mbox{Normal}(0, 100)
$$

The election term $\alpha_{e[s],d,p}$ assumes there is an initial effect of an election on the topic distribution, which then decays as a function of days since election, $d$. In particular, we model this as an AR(1) in $d$:

$$
\alpha_{e[s],d,p} = \rho_{e[s],p} \cdot \alpha_{e[s],d-1,p}
$$
One advantage of our model over using the STM is that we can restrict the effect of an election to be monotonically decreasing. This allows us to identify differences between government and election effects even when there is a one-term government. The value of the initial effect, $\alpha_{e[s],0,p}$, and the AR(1) term, $\rho$, both have non-informative priors:

$$
\alpha_{e[s],0,p} \sim \mbox{Normal}(0, 100)
$$
$$
\rho_{e[s],p} \sim \mbox{Uniform}(0,1)
$$
We model the underlying time trend in topics using splines regression. The intuition behind this term is to capture the underlying non-linear trend in topic distributions over time, which is caused by large-scale structural changes in the economy, and Australian society and culture. The $x_{s,k}$ for $k = 1,2,\dots, K$ are the value of cubic basis splines for sitting period $s$ at knot point $k$. We place knot points every five sitting periods as this is the average length of time for a government to sit. Non-informative priors are placed on the splines coefficients:
$$
\beta_{p,k} \sim \mbox{Normal}(0, 100)
$$

Finally, the sitting period-specific random effect $\delta_{s,p}$ allows for the topic distributions in some sitting periods to be different than expected based on government and election effects. This allows us to identify large deviations away from the expected distribution, thus helping to identify the effect of other, non-government and non-election events. In addition, this set up also partially pools effects across sitting periods. The $\delta_{s,p}$ values are modelled as:

$$
\delta_{s,p} \sim \mbox{Normal}(0, \sigma_{e[s],p}^2)
$$

The variance parameters $\sigma_{e[s],p}^2$ give an indication of the how the variation in topics is changing over election periods. If the estimates of the variance are larger, then there is more variation in the topics discussed within an election period. Non-informative priors are placed on the variance parameters:

$$
\sigma_{e[s],p} \sim \mbox{Uniform}(0,3)
$$

We run the model in JAGS using the \texttt{rjags} package of @Plummer2018. 

<!-- __[An illustration of the validity of our analysis model using simulated data is in Appendix \@ref(modeldetailsandsimulation).] __ -->




<!-- RA STUFF: -->

<!-- The full model is: -->

<!-- \begin{align} -->
<!-- \gamma_{stp} &\sim \mbox{Dirichlet}(\mu_{stp}) (\#eq:distributionforgammas) \\ -->
<!-- \log \mu_{stp} &= \alpha_{sp} + \delta_{stp} (\#eq:meanforgammas) \\ -->
<!-- \alpha_{s1} &= 0 (\#eq:zeroconstraint)\\ -->
<!-- \alpha_{sp} &\sim \mbox{Normal}\left(\eta_{g[s]}, \sigma^2_g\right) \mbox{ for } p>1 (\#eq:alphasp) \\ -->
<!-- \delta_{s,t,p} &\sim \mbox{Normal}\left(\delta_{s,t-1,p}, \sigma^2_{\delta}\right) (\#eq:deltasp) \\ -->
<!-- \sigma^2_{\delta} &\sim \mbox{Uniform}(0, 40). (\#eq:randomwalkvariance) -->
<!-- \end{align} -->

<!-- **[Does the variance parameter used in Equation 4 need to be specified too?]** -->

<!-- Equation \@ref(eq:distributionforgammas) describes the proportion of each day within a sitting period given to a particular topic. This is considered as a draw from a Dirichlet distribution parameterised by $\mu_{stp}$. In a sense, $\mu_{stp}$ will be mean topic distributions over whatever time period is of interest. -->

<!-- Equation \@ref(eq:meanforgammas) describes that distribution parameter, $\mu_{stp}$, as the sum of $\alpha_{sp}$, which are sitting period specific topic distributions and mean the model is linear in time for each topic and sitting period, and $\delta_{stp}$, which is a simple time series that allows for a little variation within a sitting period. These are themselves parameters. The primary aim of the modelling task is to find an appropriate and meaningful functional form for $\mu$. -->

<!-- Equation \@ref(eq:zeroconstraint) constrains the first intercept to zero for identification purposes. Equation \@ref(eq:alphasp) shows how we consider $\alpha_{sp}$ as a draw from a Normal distribution with a mean that depends on **[SOMETHING]** and a variance that depends on **[SOMETHING ELSE]**. Similarly, Equation \@ref(eq:deltasp) show how we model $\delta_{stp}$  as a draw from a Normal distribution with a mean that depends on the preceding $\delta$ and some variance which is drawn from the Uniform distribution in Equation \@ref(eq:randomwalkvariance), that is, a random walk around each intercept. -->

<!-- **[Probably need to add: 1) where does each covariate come into it and how do they interact; 2) how does correlation work; 3) what are the random effects and fixed effects?]** -->



<!-- We currently consider events in two ways. The first is graphically, grouping each of the daily topic proportions by the events and isolating the topics. This illustrates the differences between the topics. The second is by including incrementing variables in the prevalence metadata This groups each day by events and we can then conduct significance tests. Unfortunately both of these do not consider short-term effects. -->


<!-- **[Use tf-idf between them as illustrative instead? Use that other measure of difference?]** -->


<!-- We consider events in two ways. The first is in the change in the word usage before and after events and the second is in changes in the topics. -->

<!-- Differences in word usage can be evaluated using the term frequency-inverse document frequency (tf-idf) measure. It will be higher for words that are rarely used across all documents, but commonly used in a document. For instance, in Australian parliamentary text records 'the' is commonly used in many documents and so the fact that it is used in any particular document is not usual. However, 'aboriginal' is less common across, and so if it was especially prevalent in a particular document that may distinguish that document. -->

<!-- When we consider events using tf-idf, we gather terms from more than one day. We define groups of days that are roughly analogous to sitting periods. If there is more than a week between a day then we define a new group, otherwise the day is in the existing group. We use various measures **(Text2vec R package has some? or some other package)** to define a baseline measure of how different each group is, and then test for whether groups separated by our events are significantly different to this baseline. -->



# Results

Firstly, we describe the results of the CTM approach, which defined 40 unique topics over the period 1901-2017. We then describe the results of the Bayesian analysis model, which identified governments, elections and other events that were associated with a change in the topics discussed. 

## Topic modelling

We applied the CTM approach discussed in Section 3.2 on the processed Hansard text database outlined in Section 2. The main output of interest are the types of topics identified by the model, and the prevalence of each topic on of each day of parliamentary discussion. 

The remainder of the results refer to a topic model that had 80 distinct topics defined. The choice of 80 topics was made as a trade-off between the standard diagnostic tests that suggested a larger number of topics would be more appropriate, and the need for the topics to be tractable for our analysis model and understandable for us. The diagnostic tests are detailed in Appendix \@ref(selecttopicnumber).

Table \@ref(tab:topwords) lists the top ten words associated with each of the 80 topics. The topics cover areas such as budgets, transport and infrastructure, war and conflict, health, education, agriculture, and trade. Note that some topics seem to somewhat overlap with their content: for example, topics 12, 17, 22 and 23 all relate to war and conflict. **[UPDATE THAT.]** 

```{r topwords, echo = FALSE, results = 'asis'}

top_words %>% 
  kable(booktabs = T, longtable = TRUE, caption = "Top words associated with each topic") %>%
  kable_styling(font_size = 8, latex_options = c("repeat_header"))

```

## Analysis model

We are interested in considering the effect of various political and other events on what is talked about in the Australian Federal Parliament. As discussion in Section 3.3, the next stage of the modelling process takes the topic proportions estimated in the CTM approach, and models the association between these topic distributions and outside events. 

There are several outputs of interest from this modelling stage. For example, the model provides estimates of topic prevalence by each sitting period. This nicely illustrates how the topics change over time, as the daily estimates tend to be quite variable, but using periods defined by governments or elections tend not to provide enough variability. For instance, examining Topics 12, 17, 22 and 23, which have to do with war and conflict illustrates Australia's involvement in World War I, World War II, the Korean War, the Vietnam War, the first Gulf War, and the war in Afghanistan and the second Gulf War (Figure \@ref(fig:wartopicsgraph)).

```{r wartopicsgraph, cache = TRUE, echo = FALSE, fig.cap = "Model estimates of topic prevalance by sitting period for those related to war and conflict", out.width = '100%'}
knitr::include_graphics("../../figures/war_stack.pdf")
```

One of the main goals of the analysis model is to see which political events are associated with changes in the prevalence of topics over time. Political events are those related to a change of government or an election. As Australia has a parliamentary system, it is possible for the government to change without an election. We define a government based on who is the prime minister, and do not distinguish between terms or cabinet composition as is sometimes done. If a person was prime minister more than once then these are considered separately. 

As detailed in Section 3.3, the model estimates a mean level effect for each government, $\alpha_g$ and each election, $\alpha_e$. We identify differences between neighbouring governments and between neighbouring elections based on calculating 95 per cent credible intervals based on posterior samples of these respective mean effects. When these do not overlap we consider that the model finds a significant difference between either the neighbouring governments or elections. 

We summarise our results in terms of governments in Table \@ref(tab:governmentsresults) and in terms of elections in Table \@ref(tab:electionsresults). These tables focus on elections and governments that were different to the ones that preceded them. 

```{r electionsresults, echo = FALSE, results = 'asis'}

elections_results %>%
  kable(booktabs = T, caption = "Significant elections, and neighours") %>%
  kable_styling(font_size = 10)

```


```{r governmentsresults, echo = FALSE, results = 'asis'}

governments_results %>%
  kable(booktabs = T, caption = "Governments that were significantly different to their predecessor, and neighbours") %>%
  kable_styling(font_size = 10)

```

In Figures \@ref(fig:mugov) and \@ref(fig:muelection) we focus on certain topics to illustrate differences between governments and elections, respectively. In the graphs, the points show the estimated value of $\alpha_g$ and $\alpha_e$, respectively, for each of the topics specified. The error bars represent 95 per cent Bayesian credible intervals. The topics that we have focused on here are:
12, 17, 22, 23 which have to do with conflict, defence and security; 13 and 32 which have to do with commerce and trade; 24 and 33 which have to do with the budget, tax and spending; 29 which is to do with education and 39 which has to do with health.


```{r mugov, cache = TRUE, echo = FALSE, fig.cap = "Government level effects on selected topics.", out.width = '100%'}
knitr::include_graphics("../../figures/mu_gov.pdf")
```


```{r muelection, cache = TRUE, echo = FALSE, fig.cap = "Elections level effects on selected topics", out.width = '100%'}
knitr::include_graphics("../../figures/mu_election.pdf")
```


Although we do not explicitly include them in the model, the non-political events that we look for when considering periods of significant change are defined by substantial changes in various economic measures, such as the onset of the Great Depression or floating the currency; events of a historical magnitude, such as entering into a war or the 9/11 attacks; or events that had a significant effect on Australian life, such as hosting the Olympics, or the Mabo decision. The full list of events that we consider are detailed in Appendix \@ref(eventdetails). Note that we consider each chamber separately and future work could expand the model to better understand, and allow, for correlation between them.

We estimate sitting-period level-effects (essentially a mean for each topic by sitting period). The difference between this mean distribution and a particular day's topic distribution defines a measure that can be thought of as essentially a residual which allows us to identify outlying days. This approach means that the model generates dates that are interesting without us having to specify interesting dates. 

More specifically, we define a day to be 'outlying' or 'different-to-expected' if the topic distribution on that particular day is more than three standard deviations different to the mean topic distribution for the relevant sitting period. Table \@ref(tab:keyeventsresultstable) summarises the days where parliamentary discussion was significantly different from the rest prevailing in that week.

```{r keyeventsresultstable, echo = FALSE, results = 'asis'}

key_events_results %>%
  kable(booktabs = T, caption = "Days that were significantly different to other days in their sitting period") %>%
  kable_styling(font_size = 10)

```


<!-- Finally, Figure \@ref(fig:sigmaelection) illustrates the estimated standard deviation of topic distributions, around the mean election effect, over time. In general, the estimates of standard deviation are increasing over time, suggesting that discussed topics are more variable in more recent election periods compared to historical periods.  -->

<!-- ```{r sigmaelection, cache = TRUE, echo = FALSE, fig.cap = "Elections level effects on selected topics", out.width = '100%'} -->
<!-- knitr::include_graphics("../../figures/sigma.pdf") -->
<!-- ``` -->




# Discussion
Of the 36 different governments over this period, we find that 14 of them are significantly different to the government that preceded them. However, two of these results -- the significance of the Page  Government and the first Menzies Government -- are likely due to the short length of the Page Government and should be ignored, leaving only 12. The earliest of these are the Second and Third Fisher Governments, which were different to the Third Deakin Government and the Cook Government respectively. To a certain extent this is likely tied up with changes due to World War I.

The Second Menzies Government, starting in 1949, is the next government that is significantly different to a predecessor. The rest of the ones that are different are concentrated in the second half of our sample, with three of them being in the past twenty years. Similarly, of the 45 general elections that have been held we find that 14 of them define periods that were significantly different to their immediate predecessor. 1974, 1980, 1990, 1998, 2004, and 2007 stand out as elections where the government did not change, but the model suggests there was considerable change in the topics discussed in parliament.

The second Menzies Government was associated with a variety of changes compared with the preceding Chifley Government. The Chifley Government had governed through the end of World War II and the difficult economic times that followed. There was also a large increase in the number of seats in the House of Representatives at the 1949 election. Many new politicians entered parliament and this changed representation may also have been partly to do with the changed distribution of discussion topics. The sixteen-year length of the second Menzies Government, and better economic conditions over this time make it understandable why parliamentary discussion would have been different. 

There were six elections within the second Menzies Government. However we do not find that any of those elections was associated with a significant change in the topics discussed. In this sense it was a period of consistency, especially when compared with shorter-term Whitlam Government, or the longer-term Howard Government. 

The Menzies Government was succeeded by the Holt Government in January 1966. This is an example where there was a change in government without an election, as the next election only happened in November 1966. We find that the Holt Government is significantly different to the Menzies Government. In Figures \@ref(fig:muelectionall) we compare the topics during the final term of the Menzies Government with the topics of the Holt Government.

```{r muelectionall, cache = TRUE, echo = FALSE, fig.cap = "Differences between various elections", out.width = '100%'}
knitr::include_graphics("../../figures/mu_election-differences.pdf")
```

The Whitlam government is interesting as we find a difference in the topics after it was first elected in December 1972, compared with its second election win in May 1974. Figure \@ref(fig:muelectionall) compares the topics that are significant in the first Whitlam term and then compares them to those in the second Whitlam government.

The Howard Government is interesting because of the significant differences between elections. For instance, each of the election periods is associated with fairly substantial differences compared with the preceding election periods, and all apart from 2001 are actually significantly different at the 95 per cent level. Figure \@ref(fig:muelectionall) compares the topics that are significant in the different Howard terms.

To a certain extent the change after the November 2001 election is expected because of the 9/11 terrorist attacks that had only occurred two months earlier, the Bali Bombings that occurred in October 2002, and the dramatic increase in the discussion related to terrorism and conflict over these years. However the change in 1998 and 2004 is more unexpected. Although the Howard Government is the second-longest serving government and commonly thought of as a period of stability because the senior ministers were consistent as well, it might be that it is better to think of the Howard Government as a combination of three or four different periods and that the Howard Government reinvented itself over this period.

One advantage of our analysis model, compared with using the STM approach is that we can create a measure that is equivalent to testing for outliers in a model where the underlying variables were not latent. The results of this reduction in supervision are promising, but suggest specifics of our process need further refinement. For instance, our approach appropriately identifies the sitting day that first follows 11 September 2001 and 12 October 2002, which were the dates of the 9/11 Attacks and Bali Bombings respectively. It also appropriately identifies some of other key events that we were interested in. However there are many dates that we would have expected to be identified that were not, and similarly some of the dates that were identified are surprising. Further work is needed to improve this approach. For instance, our approach may not be appropriately considering step-changes.



# Conclusion
In this paper we consider what was said in the Australian Federal Parliament between 1901 and 2017. We download and parse PDFs of Hansard to create a new dataset of text. We use a correlated topic model to group the parliamentary discussion into topics to reduce the dimensionality, and then analyse the effect of various events on the distribution of these topics using a Bayesian hierarchical Dirichlet model. In general we find that changes in government change the distribution of topics discussed in parliament, but that most elections did not. We find that significant events such as 9/11 and the Bali Bombings had a substantial effect, but that with certain exceptions, economic events did not.

By bringing a new dataset of what was said in the Australian Federal Parliament to bear for our analysis we are able to consider events over the full history of the Commonwealth of Australia. However even after cleaning the dataset remains imperfect and is more fit-for-purpose than of broad applicability.

Using text as data allows larger-scale analysis that would not be viable using less-automated approaches and so it can identify associations and patterns that may otherwise be overlooked. That said, text analysis has well-known shortcomings and weaknesses and should be considered a complement to more detailed analysis such as qualitative methods and case studies. For instance, the use of topic models is problematic because of the need to interpret the topics, which at can sometimes feel like interpreting hieroglyphics. This can be difficult especially when the number of topics is large, but in a dataset of the size that we have, a large number of topics is needed. Also, although topic modelling is an unsupervised machine learning technique, the inputs require fine-tuning. For instance, selecting stop words for removal and which word to merge because of common co-location has an impact on the topics. Even after doing this there tend to be topics that are not overly meaningful, especially on their own. One way to get around using topic models is to use the words more directly, for instance word2vec and other approaches. As computational power become cheaper and more appropriate analytical methods, such as @Taddy2015 as applied in @GentzkowShapiroTaddy2018, are developed this becomes a more feasible options and future research could take that approach.

In terms of the analysis model which relates the topic distributions with events, there are several limitations to the model. Firstly, we are assuming that the effect of a particular government is constant across the whole period. In addition, we assume that the effect of elections is monotonically decreasing across days since election. In future work we will investigate different functional forms on both of these effects, and in particular try to allow for elections to have a 'lead-up' effect.  

The way that we identify unusual periods could also be improved. We defined sitting periods in a constant fashion across the whole dataset time frame, but the nature of how long parliaments sit for has most likely changed. In addition, more work needs to be done on how to identify outlying events. For example, it is not clear if an important event that occurs outside a sitting period would be identified. And if an event happens in the middle of an sitting period, it may have a large effect on the overall mean, such specific days are not identified as significantly outlying.

Finally, the current modelling and analysis set-up is a two-stage process: we take the output of a topic model, and use this as the input to a second model. However, this approach does not appropriately propagate uncertainty in topic distribution estimation. Future methodological work will focus on how to combine these two modelling steps by extending the STM approach into a more flexible framework. 

Watching our politicians at work can sometimes be a little disheartening. It can be hard to believe that not only are those in charge shouting insults that would not be tolerated in a schoolyard, but that we voted to put them there. Nonetheless, our work suggests that reasonable debate of important topics does occur in parliament. It is easy to look back and think that we live in uniquely tumultuous times, but our analysis suggests events have always driven debate and that periods of stability may be the exception. However, we do find that since the 1980s differences between government and election periods are greater than they used to be. It is, of course, reasonable that a government should act on the mandate they receive, but it takes a long time to get policy right, and we may find progress being held back if the media and political cycles become too focused on short-term events.




<!-- What could happen if we had longer terms. Eg GST needed multiple generations of politicians but carbon tax couldnâ€™t because it was one generation. -->




\newpage

# (APPENDIX) Appendix {-}

# Hansard details


## Example Hansard page {#examplehansardpage}

```{r asdf, echo=FALSE, fig.cap="Example Hansard page -- 6 February 1902", out.width = '65%', fig.align='center'}
knitr::include_graphics("../../figures/example_hansard_page.png")
```


\newpage


## Summary statistics {#hansardsummarystatistics}

The number of sitting days in a year varies considerably. The highest in the House of Representatives was 122 days in 1904, followed by 113 days in 1901 and 1920. The year with the most sitting days in the Senate was 1902 with 93 days, followed by 1989 with 92 days, and 1986 with 86 days (Figure \@ref(fig:countsbyyear)).

```{r countsbyyear, echo=FALSE, fig.cap="Number of sitting days, by year", out.width = '100%'}
knitr::include_graphics("../../figures/counts_by_year.pdf")
```

Until the 1950s the House of Representatives tended to have more sitting days than the Senate. It was then similar, before the Senate had more days in the 1980s and 1990s. Since the 2000s the House of Representatives again has tended to have more sitting days than the Senate.

These counts of the number of sitting days are based on available PDFs. For this reason the counts may be slightly different to other counts. An example of one known issue of this type is detailed in the next section.


\newpage


## Compared with parliamentary website {#knownhansardissues}
The parliamentary website provides a summary table of the number of sitting days in each year by chamber.[^parliamentarywebsitelink] Comparing the numbers provided in that table with number of days that we have provides an indication of how complete our dataset is.

[^parliamentarywebsitelink]: As at 5 November, the website was available at: https://www.aph.gov.au/Parliamentary_Business/Statistics/Senate_StatsNet/General/sittingdaysyear. 

In general the number of sitting days on the parliamentary website summary table is similar to the number of PDFs that we have although it does identify a few particularly concerning years (Figure \@ref(fig:differencesbyyear)).

```{r differencesbyyear, echo=FALSE, fig.cap="Example Hansard page - 6 February 1902", out.width = '100%'}
knitr::include_graphics("../../figures/differences_by_year.pdf")
```

When the difference is positive, it means that in that year we have fewer PDFs than the parliamentary website claims. For instance, 5 could mean that the parliamentary website claimed there were 100 days, but we only had PDFs for 95 days. Similarly, when the difference is negative then we have more PDFs than the parliamentary website claims there were sitting days.

The two major years of concern are 1992 in the House of Representatives where we have 15 days more than the parliamentary website claims there were, and 1988 in the Senate where we have eight days fewer.

The Parliament website is missing the Hansard PDFs for the following dates in the Senate: 
1988-12-21, 1988-12-20, 1988-12-19, 1988-12-16, 1988-12-15, 1988-12-14, 1988-12-13, 1988-12-12, 2000-10-12, 2000-06-19, 2004-08-09, .

There are two unaccounted for differences in 2006, one unaccounted for difference in 2001.


The Parliament website is missing the Hansard PDFs for the following dates in the House of Representatives: 
1985-08-23,
1992-09-10,
1996-12-13,
2002-05-14,
2000-10-12, and
2000-06-29.

There is one unaccounted for differences in 1920, 1921, 1935, 1942, 1948, 1951, 1991, 2003, 2004, and there are two unaccounted for in 1985 and four unaccounted for in 2012

In the Senate, the PDF for 10 August 1917 may be the wrong one? It says it is for 10 Jan 1918 on the cover sheet, but there's not even an entry for 10 Jan 1918...??? Very odd....

Senate Hansard - 1918-12-18 has the wrong PDF so does 1917-08-01.


\newpage


## Example Hansard PDF to text record workflow {#hansardworkflow}
The exact scripts that we use are too long to provide here but are available on request. Instead here we provide shorterned scripts for the workflow to convert the PDFs to usable text datasets that may be a useful starting point for other researchers. The scripts are primarily based on: the \texttt{PDFtools} R package of @Ooms2018pdftools; the \texttt{tidyverse} R package of @Wickham2017; the \texttt{tm} R package of @FeinererHornik2018; the \texttt{lubridate} R package of @GrolemundWickham2011; the \texttt{tidytext} R package of @SilgeRobinson2016; and the \texttt{stringi} R package of @Gagolewski2018. The functions of those packages are augmented by: the \texttt{furrr} R package of @VaughanDancho2018; and the \texttt{tictoc} R package of @Izrailev2014. The \texttt{hunspell} R package of @Ooms2017 is used to help find spelling issues; and the \texttt{quanteda} R package of @Benoit2018 is used to compound multiword expressions. 

The first step is scrape the websites for the PDFs.

```{r scrapingexample, echo=TRUE, eval = FALSE}
library(purrr)
library(tidyverse)

# Create lists of URLs to scrape and file names to save the PDF as
data_to_scrape <- read_csv("csv_of_address_to_visit_and_save_names.csv")
# Pull out the two pieces
address_to_visit <- data_to_scrape$URL
save_name <- data_to_scrape$file_name
# Add a containing folder to the save_names
save_name <- paste0("pdfs/", save_name)
# Create that containing folder if necessary
dir.create("pdfs") 

# The function that will visit address_to_visit and save to save_name file
visit_address_and_save_PDF <-
  function(name_of_address_to_visit,
           name_of_where_to_save) {
    # Do the actual downloading of the PDF
    download.file(name_of_address_to_visit, name_of_where_to_save)
    # Helpful so that you know progress when running it on all the records
    print(paste("Done with", name_of_where_to_save, "at", Sys.time()))  
    # Space out your requests so you don't overwhelm the website
    Sys.sleep(sample(15:30, 1)) 
  }

# Make the function more resistent to issues
safe_visit_address_and_save_PDF <- safely(visit_address_and_save_PDF)

# Walk through the lists and get the PDFs
walk2(address_to_visit, save_name, ~ safe_visit_address_and_save_PDF(.x, .y))
```


The next step is remove the front matter from the PDFs. 

```{r frontmatterexample, echo=TRUE, eval = FALSE}
library(furrr) # May need the devtools version devtools::install_github("DavisVaughan/furrr")
library(lubridate)
library(pdftools)
library(stringi)
library(tidyverse)
library(tictoc)
library(tm)
# Set up furrr to use multiple processors
plan(multiprocess)

# Create the lists of PDFs to read and file names to save text as
use_this_path_to_get_pdfs  <- "/Volumes/Hansard/pdfs/federal/hor"
use_this_path_to_save_csv_files  <- "/Volumes/Hansard/parsed/federal/hor"

# Get list of Hansard PDF filenames
file_names <-
  list.files(
    path = use_this_path_to_get_pdfs,
    pattern = "*.pdf",
    recursive = TRUE,
    full.names = TRUE
  )
file_names <- file_names %>% sample() # Randomise the order
# Modify the savenames
save_names <- file_names %>%
  str_replace(use_this_path_to_get_pdfs, "") %>%
  str_replace(".pdf", ".csv")
save_names <- paste0(use_this_path_to_save_csv_files, save_names)


#### Create the function that will be applied to the files ####
get_text_from_PDFs <-
  function(name_of_input_PDF_file,
           name_of_output_csv_file) {
    pdf_document <- pdf_text(name_of_input_PDF_file)
    # Convert to tibble so that tidyverse can be used
    pdf_document_tibble <- tibble(text = pdf_document)
    rm(pdf_document)
    # Each row is now a page of the PDF
    # Adding a column of the row numbers allows you to keep track of the page numbers
    pdf_document_tibble$pageNumbers <- 1:nrow(pdf_document_tibble)
    # Separate each line (of each page) into own row
    pdf_document_tibble <-
      separate_rows(pdf_document_tibble, text, sep = "\\n")

    # Identify page headers and footers and remove them
    pdf_document_tibble <- pdf_document_tibble %>%
      group_by(pageNumbers) %>%
      mutate(lineNumber = 1:n()) %>% # This gives you a line numbering for each page
      mutate(lastLine = n()) %>% # This tells you the number of the last line in each page
      ungroup()
    
    ## Identify front matter and remove it
    # Primarily identify the start of talking based on the first occurence of 'Mr SPEAKER'. 
    # It seems pretty common, but there are some misses (e.g. 1991-01-22). 
    # Use various backups
    pdf_document_tibble <- pdf_document_tibble %>%
      mutate(
        firstSpeakerRow = str_detect(text, "SPEAKER"),
        firstPresidentRow = str_detect(text, "PRESIDENT"),
        firstJointHouseRow = str_detect(text, "JOINT HOUSE"),
        firstTookTheChairRow = str_detect(text, "took the chair"),
        firstTheChairAtRow = str_detect(text, "the chair at")
      )
    pdf_document_tibble$firstSpeakerRow[pdf_document_tibble$firstSpeakerRow == FALSE] <- NA
    pdf_document_tibble$firstPresidentRow[pdf_document_tibble$firstPresidentRow == FALSE] <- NA
    pdf_document_tibble$firstJointHouseRow[pdf_document_tibble$firstJointHouseRow == FALSE] <- NA
    pdf_document_tibble$firstTookTheChairRow[pdf_document_tibble$firstTookTheChairRow == FALSE] <- NA
    pdf_document_tibble$firstTheChairAtRow[pdf_document_tibble$firstTheChairAtRow == FALSE] <- NA
    
    # Get the row and corresponding page and then filter to only pages from that page
    row_of_first_SPEAKER <-
      pdf_document_tibble$firstSpeakerRow[pdf_document_tibble$firstSpeakerRow == TRUE] %>% which() %>% first()
    row_of_first_PRESIDENT <-
      pdf_document_tibble$firstPresidentRow[pdf_document_tibble$firstPresidentRow == TRUE] %>% which() %>% first()
    row_of_first_JOINTHOUSE <-
      pdf_document_tibble$firstJointHouseRow[pdf_document_tibble$firstJointHouseRow == TRUE] %>% which() %>% first()
    row_of_first_TookTheChair <-
      pdf_document_tibble$firstTookTheChairRow[pdf_document_tibble$firstTookTheChairRow == TRUE] %>% which() %>% first()
    row_of_first_TheChairAt <-
      pdf_document_tibble$firstTheChairAtRow[pdf_document_tibble$firstTheChairAtRow == TRUE] %>% which() %>% first()
    
    first_page_of_interest_SPEAKER <-
      pdf_document_tibble[row_of_first_SPEAKER, "pageNumbers"] %>% as.integer()
    first_page_of_interest_PRESIDENT <-
      pdf_document_tibble[row_of_first_PRESIDENT, "pageNumbers"] %>% as.integer()
    first_page_of_interest_JOINTHOUSE <-
      pdf_document_tibble[row_of_first_JOINTHOUSE, "pageNumbers"] %>% as.integer()
    first_page_of_interest_TookTheChair <-
      pdf_document_tibble[row_of_first_TookTheChair, "pageNumbers"] %>% as.integer()
    first_page_of_interest_TheChairAt <-
      pdf_document_tibble[row_of_first_TheChairAt, "pageNumbers"] %>% as.integer()
    
    first_page_of_interest_JOINTHOUSE <-
      (first_page_of_interest_JOINTHOUSE + 1)  %>% as.integer()
    
    filter_from_here <-
      case_when(
        !is.na(first_page_of_interest_TookTheChair) ~ first_page_of_interest_TookTheChair,
        !is.na(first_page_of_interest_TheChairAt) ~ first_page_of_interest_TheChairAt,
        !is.na(first_page_of_interest_SPEAKER) ~ first_page_of_interest_SPEAKER,
        !is.na(first_page_of_interest_PRESIDENT) ~ first_page_of_interest_PRESIDENT,
        TRUE ~ first_page_of_interest_JOINTHOUSE
      )

    pdf_document_tibble <- pdf_document_tibble %>%
      filter(pageNumbers >= filter_from_here) %>%
      select(-firstSpeakerRow,
             -firstPresidentRow,
             -firstJointHouseRow,
             -firstTookTheChairRow,
             -firstTheChairAtRow)
    
    # Save file
    write_csv(pdf_document_tibble, name_of_output_csv_file)
    # Helpful to know where you're up to
    print(paste0("Done with ", name_of_output_csv_file, " at ", Sys.time()))
  }


#### Walk through the lists and parse the PDFs ####
# Avoid errors
safely_get_text_from_PDFs <- safely(get_text_from_PDFs)

tic("Furrr walk2 stringr")
future_walk2(file_names,
             save_names,
             ~ safely_get_text_from_PDFs(.x, .y),
             .progress = TRUE)
toc()

```

UP TO HERE

**[TBD].**


\newpage


## Stopwords over time {#stopwordsgraph}
Insert the graph of stop words over time. 


\newpage


# Topic modelling example and details {#LDAexample}

## Overview and example
As applied to Hansard, LDA considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the politician then chooses appropriate words to use for each of those topics. Statistically, LDA considers each document as having been generated by some probability distribution over topics. Similarly, each topic is considered a probability distribution over terms. To choose the terms used in each document, terms are picked from each topic in the appropriate proportion. 

As an example, Figures \@ref(fig:topicsoverdocuments) and \@ref(fig:topicsoverterms) illustrate a smaller application with five topics, two documents, and ten terms. In this case, the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \@ref(fig:topicsoverdocuments)).

```{r topicsoverdocuments, cache = TRUE, echo=FALSE, fig.cap = "Probability distributions over topics for two documents", fig.height = 4}

tibble(
  Topics = rep(c("1", "2", "3", "4", "5"), 2),
  Probability = c(c(0.40, 0.40, 0.1, 0.05, 0.05), c(0.01, 0.04, 0.35, 0.20, 0.4)),
  Document = c(rep("Document 1", 5), rep("Document 2", 5))
  ) %>%
  ggplot(aes(Topics, Probability, colour = Document)) +
  geom_point() +
  theme_classic() +
  coord_flip()  +
  scale_colour_viridis_d()

```

For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \@ref(fig:topicsoverterms)).

```{r topicsoverterms, cache = TRUE, echo=FALSE, fig.cap = "Probability distributions over terms", fig.height = 4}
tibble(
  Terms = rep(c("migration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"), 2),
  Probability = c(c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2), c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)),
  Topics = c(rep("Topic 1", 10), rep("Topic 2", 10))
  ) %>%
  ggplot(aes(Terms, Probability, colour = Topics)) +
  geom_point() +
  theme_classic() +
  coord_flip() +
  scale_colour_viridis_d()

```

\newpage

## Document generation process

Following @BleiLafferty2009, @blei2012 and @GriffithsSteyvers2004, the process by which a document is generated is more formally considered to be:

1. There are $1, 2, \dots, k, \dots, K$ topics and the vocabulary consists of $1, 2, \dots, V$ terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the $k$th topic is $\beta_k$. Typically a topic would be a small number of terms and so the Dirichlet distribution with hyper-parameter $\boldsymbol{\eta}$ is used: $\beta_k \sim \mbox{Dirichlet}(\boldsymbol{\eta})$, where $\boldsymbol{\eta} = (\eta_1, \eta_2, \dots, \eta_{K})$.[^Dirichletfootnote] In practice, a symmetric Dirichlet distribution is usually used, where all elements of $\boldsymbol{\eta}$ are equal. 
2. Decide the topics that each document will cover by randomly drawing distributions over the $K$ topics for each of the $1, 2, \dots, d, \dots, D$ documents. The topic distributions for the $d$th document are $\theta_d$, and $\theta_{d,k}$ is the topic distribution for topic $k$ in document $d$. Again, the Dirichlet distribution with the hyper-parameter $0<\alpha<1$ is used here because usually a document would only cover a handful of topics: $\theta_d \sim \mbox{Dirichlet}(\boldsymbol{\alpha})$. Again, strictly $\boldsymbol{\alpha}$ is vector of length $K$ of hyper-parameters and they are usually equal.
3. If there are $1, 2, \dots, n, \dots, N$ terms in the $d$th document, then to choose the $n$th term, $w_{d, n}$:
    a. Randomly choose a topic for that term $n$, in that document $d$, $z_{d,n}$, from the multinomial distribution over topics in that document, $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$.
    b. Randomly choose a term from the relevant multinomial distribution over the terms for that topic, $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$.

[^Dirichletfootnote]: The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, where all elements of $\eta=1$, it is equivalent to a uniform distribution. If $\eta<1$, then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as $\eta$ decreases. A hyper-parameter is a parameter of a prior distribution.

Given this set-up, the joint distribution for the variables is (@blei2012, p.6):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$

Based on this document generation process the analysis problem, discussed next, is to compute a posterior over $\beta_{1:K}$ and $\theta_{1:D}$, given $w_{1:D, 1:N}$. This is intractable directly, but can be approximated (@GriffithsSteyvers2004 and @blei2012).

After the documents are created, they are all that we have to analyse. The term usage in each document, $w_{1:D, 1:N}$, is observed, but the topics are hidden, or 'latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \@ref(fig:topicsoverdocuments) or \@ref(fig:topicsoverterms). In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (@SteyversGriffiths2006). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (@blei2012, p. 7):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$

<!-- The initial practical step when implementing LDA given a collection of documents is to remove 'stop words'. These are words that are common, but that don't typically help to define topics. There is a common list of stop words such as: "a"; "an"; and "and". However the exact list used depends on research of focus. In the case of Australian Hansard, these are words such as: "australia"; "australian"; and "bill". Punctuation and capitalisation is also typically removed. The documents then need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document. -->

<!-- After the dataset is ready, the R \texttt{topicmodels} package of @Grun2011 can be used to implement LDA and approximate the posterior.  -->

Gibbs sampling or the variational expectation-maximization algorithm can be used to approximate the posterior. A summary of these approaches is provided next. 

\newpage

## Posterior estimation {#LDAposteriorestimation}
Following @SteyversGriffiths2006 and @Darling2011, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with $\alpha = \frac{50}{K}$ and $\eta = 0.1$  (@SteyversGriffiths2006 recommends $\eta = 0.01$), where $\alpha$ refers to the distribution over topics and $\eta$ refers to the distribution over terms (@Grun2011, p. 7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (@Grun2011, p. 6):
$$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$
where $z'_{d, n}$ refers to all other topic assignments; $\lambda'_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$; $\lambda'_{.\rightarrow k}$ is a count of how many other times that any term has been assigned to topic $k$; $\lambda'^{(d)}_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$ in that particular document; and $\lambda'^{(d)}_{-i}$ is a count of how many other times that term has been assigned in that document. Once $z_{d,n}$ has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (@SteyversGriffiths2006). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.

The choice of the number of topics, *k*, drives the results and must be specified *a priori*. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use cross validation. More detail on this process is provided in the next section.



\newpage

## Selection of number of topics {#selecttopicnumber}

The choice of the number of topics to use in a topic model has a substantial affect on the results of the model. For instance, in our topic model, choosing a smaller number of topics, such as 10 or 20 results in a model that is not all that useful because the topics are so broad.

There are a variety of diagnostic measures that can guide the selection of the topics, but there is rarely an obvious best choice, especially at a more fine level such as choosing betweeen 60 and 65 topics. We found it useful to try a few quite different measures before settling on 80 topics. This provided a balance between being granular enough to be informative--anything less than 40 topics tended to be too broad-- yet still being tractable for our analysis model in a reasonable amount of time. In addition to looking at the topics and how they changed over time, diagnostic measures that we considered include the held-out likelihood, the lower bound, residuals, exclusivity. and semantic coherence (Figures \@ref(fig:topicsdiagnostics)).[^Silgefootnote] 

[^Silgefootnote]: The code for creating the figures in this section is based on @Silge2018.

```{r topicsdiagnostics, cache = TRUE, echo = FALSE, fig.cap = "Model diagnostics", out.width = '100%'}
knitr::include_graphics("../../figures/diagnostics_for_k.pdf")
```

@RobertsStewartAiroldiRPackage provides more detail about the diagnostic tests that we use, but we briefly discuss each here. Exclusivity is a measure of how specific words are to particular topics. It looks at the proportion that a word makes up of a particular topic compared with the proportion that word makes up of the other topics. As the number of topics increases we usually expect exclusivity to increase because the topics become more particular. Higher values are better. The held out likelihood as described by @Wallach2009 takes a test/training approach to estimate the probability of held-out documents given the training documents. Higher values are better. The lower bound gives some indication of whether the model may have multiple modes and hence the end result be sensitive to the starting position (@Roberts2016navigating). Residuals analysis (@Taddy2012) compares the theoretical distribution of the variance with the eactual distribution. It is a test for overdispersion of the variance, and if it is found then this can suggest that more topics would be appropriate. Semantic coherence is the tradeoff for having topics that are more specific and the subsequent risk that the topics become meaningless. @Mimno2011 define a measure of coherence that is based on ratios of single words compared with pairs of words. The idea is that words that should occur in the one document should be more likely to be in a particular topic than ones that do not occur together. For instance, a topic that has 'wine' and 'cheese' as highly rated words would score better on their measure than another that contained 'cheese' and 'mining'. Lower values are better. @RobertsStewartAiroldiRPackage recommend examining the tradeoff between exclusitivity and semantic coherence. This suggests that the magnitude of improvement reduces from about 80 topics (Figure \@ref(fig:topicsexclusivityvscoherence)).

```{r topicsexclusivityvscoherence, cache = TRUE, echo = FALSE, fig.cap = "Exclusitivity compared with semantic coherence", out.width = '100%'}
knitr::include_graphics("../../figures/exclusivity_vs_coherence.pdf")
```




<!-- \newpage -->


<!-- ## Robustness of results -->
<!-- **[IS THIS BEING USED?]** -->
<!-- Here we change the number of sitting days considered either side of an event. The results in the main section of the paper are for the nearest ten days either side of an event. Here are show that the results are essentially the same if the nearest one, two, five, and twenty days either side of an event. -->



\newpage


# Events {#eventdetails}

```{r governments, echo = FALSE, results = 'asis'}

governments %>% 
  kable(booktabs = T, caption = "Change in governments") %>%
  kable_styling(font_size = 10)

```


```{r elections, echo = FALSE, results = 'asis'}

elections %>% 
  filter(election == 1) %>% 
  select(year, electionDate, electionWinner) %>% 
  kable("latex", booktabs = T, caption = "Elections") %>%
  kable_styling(font_size = 10)

```


```{r keyevents, echo = FALSE, results = 'asis'}

keyEvents %>% 
  kable(booktabs = T, caption = "Key events") %>%
  kable_styling(font_size = 10)

```


For background on the Premiers' Plan see @Copland1934.

For background on the Gruen tariff cut see @Gruen1975.

Add the graphs and procedures. **[TBD]**


\newpage


<!-- # Analysis model {#modeldetailsandsimulation} -->


<!-- Here we illustrate the validity of our analysis model using simulated data. -->

<!-- \newpage -->


# References

