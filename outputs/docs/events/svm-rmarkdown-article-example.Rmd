---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
title: "Nobody expects the Spanish Inquisition: Which events drive topic change in Australian parliaments?"
thanks: "Thank you to John Tang, Zach Ward, Tim Hatton and Martine Mariotti for their helpful comments, guidance and suggestions. **Version as of**: `r format(Sys.time(), '%B %d, %Y')`; **Comments welcome**: rohan.alexander@anu.edu.au"
author:
- name: Monica Alexander
  affiliation: University of Toronto
- name: Rohan Alexander
  affiliation: Australian National University
abstract: "We use a structural text model to explore the effect of various events on what was said in Australian state and federal parliaments from the mid-1800s through to 2017. We find that: 1) changes of government are associated with topic changes only when there is also a change in the party in power; 2) polling results appear dissociated from parliamentary topics; 3) economic changes, such as financial crises have a significant effect; and 4) other events, such as an unexpected attack tend not to have a prolonged change."
keywords: "text analysis, politics, Australia"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
# spacing: double
bibliography: ../bibliography.bib
biblio-style: apsr
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```


# Introduction
New governments often go to some trouble to be different from the governments they replace. For instance, Kevin Rudd's apology to Indigenous Australians was not supported by John Howard, and one of Tony Abbott's first acts was to repeal Rudd's carbon tax. Similarly, significant events can alter the course of a government. For instance, consider the change in the Howard government after the 9/11 attacks or the 2002 Bali bombings. However it is not so clear which events drive changes in topics, for instance, do they change when the government is replaced by another of its own party? And which events are temporary, for instance when an economic crisis abates, do the topics return to pre-crisis levels?

In this paper we use the Structural Topic Model (STM) of @RobertsStewartAiroldi2016 to model the topics of discussion in Australian parliaments. The advantage of this model is that it allows for topics to be correlated between sitting days, which then allows us to test for changes in topics at various events. The events that we focus on are changes in: 1) government; 2) the political environment (as defined by polling or other results); 3) economic conditions; and 4) the significant events (such as the 9/11 attacks or the Bali Bombings).

We find [INSERT RESULTS]. We also explored the other direction (the impact of what was said in parliaments on events) but it was difficult to find significant effects.

Our work fits into [WHATEVER IT FITS INTO]. While using text as data has well-known shortcomings, it allows larger-scale analysis that would not be viable using less-automated approaches and hence can identify patterns that may otherwise be overlooked.

<!-- What events changed the course of topics in Parliament, and what was the pattern before? Eg What did we miss out on because of 9/11, or an assassination? Combine them with the average length that it takes to get legislation through to be specific about what stopped. Similarly, how does the topics change after a change of government? Eg What could have happened if Kevin 07 didn’t kick Howard out. -->

# Data
Following the example of the UK a text record called Hansard of what was said in Australian parliaments has been made available since their establishment.[^1] Hansard records are an increasingly popular source of data as new methods and reduced computational costs make larger-scale analysis easier. For instance, the digitisation of the Canadian Hansard, @BeelenEtc2017, allowed @Whyte2017 to examine whether parliamentary disruptions in Canada increased between 1926 and 2015. In the UK, @Duthie2016 analysed Hansard records to examine which politicians made supportive or aggressive statements toward other politicians between 1979 and 1990, and @Willis2017 examined how politicians understood climate change. In New Zealand, @Curran2017 modelled the topics discussed between 2003 and 2016, and @Graham2016 examined unparliamentary language between 1890 and 1950. 

Australian Hansard records have been analysed for various purposes, but usually not at scale. For instance, @Rasiah2010 examines Hansard records for the Australian House of Representatives to examine whether politicians attempted to evade questions about Iraq during February and March 2003. @GansLeigh2012 examined Australian Hansard records by hand to associate mentions by politicians of certain public intellectuals with neutral or positive sentiment. 

[^1]: While Hansard is not necessarily verbatim, it is considered close enough for text-as-data purposes. For instance, @Mollin2008 found that in the case of the UK Hansard the differences would only affect specialised linguistic analysis. @Edwards2016 examined Australia, New Zealand and the UK, and found that changes were usually made by those responsible for creating the Hansard record, instead of the parliamentarians. Both these findings provide reassurance that differences between Hansard and a verbatim record would not be meaningful for this paper. 

The Australian parliaments generally make their Hansard records available online as PDFs that can be downloaded. The Federal parliament additionally makes XML records available for years between 1901 and 1980 as well as from 1997. There are roughly 65,000 **(UPDATE)** hansard records available across the chambers of the state and federal parliaments (Table 1) **(UPDATE NUMBERING)**. As with any larger-scale data process, there are many issues with this dataset of PDFs and the known ones are detailed in the Appendix.

| Parliament | House | Years used | Notes |
|:------|-----:|---------:|------:|
| Commonwealth | House of Representatives | 1901 - 2017 | - |
| Queensland | ? | 1861 - 2017 | - |
| New South Wales | ? | ? - 2017 | - |
| Victoria | ? | ? - 2017 | - |
| Tasmania | ? | ? - 2017 | - |
| South Australia | ? | ? - 2017 | - |
| Western Australia | ? | ? - 2017 | - |

The PDFs were processed using the \texttt{PDFtools} R package of @Ooms2018pdftools. A small proportion of the Hansard records had not been put through a professional OCR process and although the Google Tesseract engine as implemented by @Ooms2018tesseract provided some useful data, these were not used in this analysis. 


to extract these as text-based CSV records, and clean the records using functions from the \texttt{Tidyverse} R package of @WickhamHadleyTidyverse, the \texttt{Tidytext} R package of @SilgeRobinson2016 and the [INSERT OTHERS]. The result is a 





# Model
The primary model that we use in this paper is the Structural Topic Model (STM) as implemented by the \texttt{STM} R package of @RobertsStewartAiroldiRPackage. The basis of this type of topic modelling is the Latent Dirichlet Allocation (LDA) model of @Blei2003latent. In this section a brief overview of both the LDA model and the STM approach is provided and then the specifics of how we consider events in this setting are discussed.

## Latent Dirichlet Allocation
Each day's Hansard record needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way, especially over longer time periods. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of @Blei2003latent, for instance as implemented by the R package 'topicmodels' by @Grun2011.

The key assumption behind the LDA method is that each day's text, 'a document', in Hansard is made by speakers who decide the topics they would like to talk about in that document, and then chooses words, 'terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified *ex ante*; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in each day's Hansard group themselves to define topics.

As applied to Hansard, the LDA method considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the speaker then chooses appropriate words to use for each of those topics. 

More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \@ref(fig:topicsoverdocuments)).

```{r topicsoverdocuments, cache = TRUE, echo=FALSE, fig.cap = "Probability distributions over topics"}
document_1 <- tibble(
  Topics = c(
  "topic 1", "topic 2", "topic 3", "topic 4", "topic 5"
  ),
  Probability = c(0.40, 0.40, 0.1, 0.05, 0.05)
  )

document_2 <- tibble(
  Topics = c(
  "topic 1", "topic 2", "topic 3", "topic 4", "topic 5"
  ),
  Probability = c(0.01, 0.04, 0.35, 0.20, 0.4)
  )

# par(mfrow=c(2,1))
ggplot(document_1, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 1") +
  coord_cartesian(ylim = c(0, 0.4))
ggplot(document_2, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \@ref(fig:topicsoverterms)).

```{r topicsoverterms, cache = TRUE, echo=FALSE, out.width='.49\\linewidth', fig.cap = "Probability distributions over terms", fig.show = 'hold'}
topic_1 <- tibble(
  Terms = c(
  "immigration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"
  ),
  Probability = c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2)
  )

topic_2 <- tibble(
  Terms = c(
  "immigration", "race", "influx", "loans", "wealth", "saving", "chinese", "france", "british", "english"
  ),
  Probability = c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)
  )

# par(mar = c(4, 4, 0.1, 0.1))
ggplot(topic_1, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 1") +
  coord_cartesian(ylim = c(0, 0.4))
ggplot(topic_2, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

Following @BleiLafferty2009, @blei2012 and @GriffithsSteyvers2004, the process by which a document is generated is more formally considered to be:

1. There are $1, 2, \dots, k, \dots, K$ topics and the vocabulary consists of $1, 2, \dots, V$ terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the $k$th topic is $\beta_k$. Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter $0<\eta<1$ is used: $\beta_k \sim \mbox{Dirichlet}(\eta)$.[^Dirichletfootnote] Strictly, $\eta$ is actually a vector of hyperparameters, one for each $K$, but in practice they all tend to be the same value.
2. Decide the topics that each document will cover by randomly drawing distributions over the $K$ topics for each of the $1, 2, \dots, d, \dots, D$ documents. The topic distributions for the $d$th document are $\theta_d$, and $\theta_{d,k}$ is the topic distribution for topic $k$ in document $d$. Again, the Dirichlet distribution with the hyperparameter $0<\alpha<1$ is used here because usually a document would only cover a handful of topics: $\theta_d \sim \mbox{Dirichlet}(\alpha)$. Again, strictly $\alpha$ is vector of length $K$ of hyperparameters, but in practice each is usually the same value.
3. If there are $1, 2, \dots, n, \dots, N$ terms in the $d$th document, then to choose the $n$th term, $w_{d, n}$:
    a. Randomly choose a topic for that term $n$, in that document $d$, $z_{d,n}$, from the multinomial distribution over topics in that document, $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$. 
    b. Randomly choose a term from the relevant multinomial distribution over the terms for that topic, $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$.
    
[^Dirichletfootnote]: The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, $\eta=1$, it is equivalent to a uniform distribution. If $\eta<1$, then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as $\eta$ decreases. A hyperparameter is a parameter of a prior distribution.

Given this set-up, the joint distribution for the variables is (@blei2012, p.6):
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$

Based on this document generation process the analysis problem, discussed next, is to compute a posterior over $\beta_{1:K}$ and $\theta_{1:D}$, given $w_{1:D, 1:N}$. This is intractable directly, but can be approximated (@GriffithsSteyvers2004 and @blei2012).

After the documents are created, they are all that we have to analyse. The term usage in each document, $w_{1:D, 1:N}$, is observed, but the topics are hidden, or 'latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \@ref(fig:topicsoverdocuments) or \@ref(fig:topicsoverterms). In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (@SteyversGriffiths2006). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (@blei2012, p. 7): 
$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$

The initial practical step when implementing LDA given a collection of documents is to remove 'stop words'. These are words that are common, but that don't typically help to define topics. There is a general list of stop words such as: "a"; "a's"; "able"; "about"; "above"... An additional list of words that are commonly found in Hansard, but likely don't help define topics is added to the general list. These additions include words such as: "act"; "amendment"; "amount"; "australia"; "australian"; "bill"... A full list can be found in Appendix \@ref(hansard-stop-word). We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.

After the dataset is ready, the R package 'topicmodels' by @Grun2011 can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following @SteyversGriffiths2006 and @Darling2011, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with $\alpha = \frac{50}{K}$ and $\eta = 0.1$  (@SteyversGriffiths2006 recommends $\eta = 0.01$), where $\alpha$ refers to the distribution over topics and $\eta$ refers to the distribution over terms (@Grun2011, p. 7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (@Grun2011, p. 6):
$$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$
where $z'_{d, n}$ refers to all other topic assignments; $\lambda'_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$; $\lambda'_{.\rightarrow k}$ is a count of how many other times that any term has been assigned to topic $k$; $\lambda'^{(d)}_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$ in that particular document; and $\lambda'^{(d)}_{-i}$ is a count of how many other times that term has been assigned in that document. Once $z_{d,n}$ has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (@SteyversGriffiths2006). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate. 

The choice of the number of topics, *k*, affects the results, and must be specified *a priori*. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for *k* and then picking an appropriate value that performs well.

One weakness of the LDA method is that it considers a 'bag of words' where the order of those words does not matter (@blei2012). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking. This motivates the use of the Structural Topic Model, described in the next section.


## Structural Topic Model
### Overview and example
[TBD]

Note that each of the states and the Commonwealth are treated independently here. Future work could expand the model to better understand, and allow, for correlation between them.

### Considering events
[TBD]


# Results
## Political events
>*When you change the government, you change the country.* Paul Keating.

Change of government.

## Polling events
>*The only poll that matters is the one on election day.* John Howard.

[TBD]

## Economic events

Major economic changes.

[TBD]


## External events
>*Events, dear boy, events.* Attributed to Harold Macmillan.

Major event such as 9/11 attacks, or economic change.



# Summary and conclusions
What could happen if we had longer terms. Eg GST needed multiple generations of politicians but carbon tax couldn’t because it was one generation. 

Text analysis has well-known biases and weaknesses and is a complement to more detailed analysis such as qualitative methods and case studies. We consider the results presented in this paper, as well as many of those results of the larger text-as-data research program, as fitting within findings based on other methods.





\newpage

# Appendix

## Document sources
Where from?

Which years are being used (not non-OCRd)

## Dataset issues
Which PDFs are missing or have no content, etc.

## PDF to CSV issues
Insert graph of stop words over time.

## Selection of number of topics


## Robustness of results
Here we change the number of sitting days considered either side of an event. The results in the main section of the paper are for the nearest ten days either side of an event. Here are show that the results are essentially the same if the nearest one, two, five, and twenty days either side of an event.


\newpage


# References

